{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1c1454",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure amd test GPU\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "# Prevent automatic GPU memory pre-allocation\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    print(gpu)\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "print(tf.__version__)\n",
    "# print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50afa68",
   "metadata": {},
   "source": [
    "## Implementation of GraphSAGE\n",
    "\n",
    "- Implementation was based off: https://github.com/williamleif/GraphSAGE/tree/master\n",
    "- Current code copied are implemented in a .py file, meant to be executed in the command line. Have to modify code to make it able to run in a jupyter notebook\n",
    "- Possible deprecation of Tensorflow modules like tf.contrib "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2c5e27",
   "metadata": {},
   "source": [
    "## Referenced code\n",
    "- This section shows reference code for GraphSAGE, which may be deprecated at some parts and is designed for .py and command line usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8306d9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get initialisers\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# DISCLAIMER:\n",
    "# Parts of this code file are derived from\n",
    "# https://github.com/tkipf/gcn\n",
    "# which is under an identical MIT license as GraphSAGE\n",
    "\n",
    "def uniform(shape, scale=0.05, name=None):\n",
    "    \"\"\"Uniform init.\"\"\"\n",
    "    initial = tf.random_uniform(shape, minval=-scale, maxval=scale, dtype=tf.float32)\n",
    "    return tf.Variable(initial, name=name)\n",
    "\n",
    "\n",
    "def glorot(shape, name=None):\n",
    "    \"\"\"Glorot & Bengio (AISTATS 2010) init.\"\"\"\n",
    "    init_range = np.sqrt(6.0/(shape[0]+shape[1]))\n",
    "    initial = tf.random_uniform(shape, minval=-init_range, maxval=init_range, dtype=tf.float32)\n",
    "    return tf.Variable(initial, name=name)\n",
    "\n",
    "\n",
    "def zeros(shape, name=None):\n",
    "    \"\"\"All zeros.\"\"\"\n",
    "    initial = tf.zeros(shape, dtype=tf.float32)\n",
    "    return tf.Variable(initial, name=name)\n",
    "\n",
    "def ones(shape, name=None):\n",
    "    \"\"\"All ones.\"\"\"\n",
    "    initial = tf.ones(shape, dtype=tf.float32)\n",
    "    return tf.Variable(initial, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22e490e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the layers required for the model\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from graphsage.inits import zeros\n",
    "\n",
    "flags = tf.app.flags\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "# DISCLAIMER:\n",
    "# Boilerplate parts of this code file were originally forked from\n",
    "# https://github.com/tkipf/gcn\n",
    "# which itself was very inspired by the keras package\n",
    "\n",
    "# global unique layer ID dictionary for layer name assignment\n",
    "_LAYER_UIDS = {}\n",
    "\n",
    "def get_layer_uid(layer_name=''):\n",
    "    \"\"\"Helper function, assigns unique layer IDs.\"\"\"\n",
    "    if layer_name not in _LAYER_UIDS:\n",
    "        _LAYER_UIDS[layer_name] = 1\n",
    "        return 1\n",
    "    else:\n",
    "        _LAYER_UIDS[layer_name] += 1\n",
    "        return _LAYER_UIDS[layer_name]\n",
    "\n",
    "class Layer(object):\n",
    "    \"\"\"Base layer class. Defines basic API for all layer objects.\n",
    "    Implementation inspired by keras (http://keras.io).\n",
    "    # Properties\n",
    "        name: String, defines the variable scope of the layer.\n",
    "        logging: Boolean, switches Tensorflow histogram logging on/off\n",
    "\n",
    "    # Methods\n",
    "        _call(inputs): Defines computation graph of layer\n",
    "            (i.e. takes input, returns output)\n",
    "        __call__(inputs): Wrapper for _call()\n",
    "        _log_vars(): Log all variables\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        allowed_kwargs = {'name', 'logging', 'model_size'}\n",
    "        for kwarg in kwargs.keys():\n",
    "            assert kwarg in allowed_kwargs, 'Invalid keyword argument: ' + kwarg\n",
    "        name = kwargs.get('name')\n",
    "        if not name:\n",
    "            layer = self.__class__.__name__.lower()\n",
    "            name = layer + '_' + str(get_layer_uid(layer))\n",
    "        self.name = name\n",
    "        self.vars = {}\n",
    "        logging = kwargs.get('logging', False)\n",
    "        self.logging = logging\n",
    "        self.sparse_inputs = False\n",
    "    \n",
    "    # Let subclass inherit this function and write the unique function body for the subclass\n",
    "    def _call(self, inputs):\n",
    "        return inputs\n",
    "    \n",
    "    # Purpose: Write input and output to tf.summary, to visualise change in input and output over epochs\n",
    "    \n",
    "    # Writes a histogram to the current default summary writer, for later analysis in TensorBoard's 'Histograms'\n",
    "    # and 'Distributions' dashboards (data written using this API will appear in both places)\n",
    "    def __call__(self, inputs):\n",
    "        \n",
    "        # Provides a hierarchical naming structure by adding a prefix to the names of operations and variables\n",
    "        with tf.name_scope(self.name):\n",
    "            if self.logging and not self.sparse_inputs:\n",
    "                tf.summary.histogram(self.name + '/inputs', inputs)\n",
    "            outputs = self._call(inputs)\n",
    "            if self.logging:\n",
    "                tf.summary.histogram(self.name + '/outputs', outputs)\n",
    "            return outputs\n",
    "    \n",
    "    # Log the variables obtained for each of the layer\n",
    "    def _log_vars(self):\n",
    "        for var in self.vars:\n",
    "            tf.summary.histogram(self.name + '/vars/' + var, self.vars[var])\n",
    "\n",
    "\n",
    "class Dense(Layer):\n",
    "    \"\"\"Dense layer.\"\"\"\n",
    "    \n",
    "    # Inherits parent Layer class and accept additional parameters to make the Dense layer\n",
    "    def __init__(self, input_dim, output_dim, dropout=0., \n",
    "                 act=tf.nn.relu, placeholders=None, bias=True, featureless=False, \n",
    "                 sparse_inputs=False, **kwargs):\n",
    "        super(Dense, self).__init__(**kwargs)\n",
    "\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.act = act\n",
    "        self.featureless = featureless\n",
    "        self.bias = bias\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        # helper variable for sparse dropout\n",
    "        self.sparse_inputs = sparse_inputs\n",
    "        if sparse_inputs:\n",
    "            self.num_features_nonzero = placeholders['num_features_nonzero']\n",
    "        \n",
    "        # Used to control variable sharing and reuse within a TensorFlow graph\n",
    "        with tf.variable_scope(self.name + '_vars'):\n",
    "            \n",
    "            # tf.get_variable(name) creates a new variable called name (or add _ \n",
    "            # if name already exists in the current scope) in the tensorflow graph.\n",
    "            \n",
    "            # The tf.app.flags module is a functionality provided by Tensorflow to implement\n",
    "            # command line flags for your Tensorflow program\n",
    "            self.vars['weights'] = tf.get_variable('weights', shape=(input_dim, output_dim),\n",
    "                                         dtype=tf.float32, \n",
    "                                         initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                         regularizer=tf.contrib.layers.l2_regularizer(FLAGS.weight_decay))\n",
    "            if self.bias:\n",
    "                self.vars['bias'] = zeros([output_dim], name='bias')\n",
    "\n",
    "        if self.logging:\n",
    "            self._log_vars()\n",
    "    \n",
    "    # Perform dot product on input and add bias to produce outputs\n",
    "    def _call(self, inputs):\n",
    "        x = inputs\n",
    "\n",
    "        x = tf.nn.dropout(x, 1-self.dropout)\n",
    "\n",
    "        # transform\n",
    "        output = tf.matmul(x, self.vars['weights'])\n",
    "\n",
    "        # bias\n",
    "        if self.bias:\n",
    "            output += self.vars['bias']\n",
    "\n",
    "        return self.act(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90cbdbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get aggregators\n",
    "# Will be using MeanAggregator for now\n",
    "import tensorflow as tf\n",
    "\n",
    "from .layers import Layer, Dense\n",
    "from .inits import glorot, zeros\n",
    "\n",
    "# Inherits from parent Layer class\n",
    "class MeanAggregator(Layer):\n",
    "    \n",
    "    \"\"\"\n",
    "    Aggregates via mean followed by matmul and non-linearity.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, output_dim, neigh_input_dim=None,\n",
    "            dropout=0., bias=False, act=tf.nn.relu, \n",
    "            name=None, concat=False, **kwargs):\n",
    "        super(MeanAggregator, self).__init__(**kwargs)\n",
    "\n",
    "        self.dropout = dropout\n",
    "        self.bias = bias\n",
    "        self.act = act\n",
    "        self.concat = concat\n",
    "        \n",
    "        # Give the neighbour same dimensions as current node if no dimensions provided for the neighbours\n",
    "        if neigh_input_dim is None:\n",
    "            neigh_input_dim = input_dim\n",
    "\n",
    "        if name is not None:\n",
    "            name = '/' + name\n",
    "        else:\n",
    "            name = ''\n",
    "\n",
    "        with tf.variable_scope(self.name + name + '_vars'):\n",
    "            \n",
    "            # Initialise weights and bias for this layer\n",
    "            self.vars['neigh_weights'] = glorot([neigh_input_dim, output_dim],\n",
    "                                                        name='neigh_weights')\n",
    "            self.vars['self_weights'] = glorot([input_dim, output_dim],\n",
    "                                                        name='self_weights')\n",
    "            if self.bias:\n",
    "                self.vars['bias'] = zeros([self.output_dim], name='bias')\n",
    "\n",
    "        if self.logging:\n",
    "            self._log_vars()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "    def _call(self, inputs):\n",
    "        \n",
    "        # Current node embeddings, Neighbouring node embeddings\n",
    "        self_vecs, neigh_vecs = inputs\n",
    "\n",
    "        neigh_vecs = tf.nn.dropout(neigh_vecs, 1-self.dropout)\n",
    "        self_vecs = tf.nn.dropout(self_vecs, 1-self.dropout)\n",
    "        \n",
    "        # Computes the mean of elements across dimensions of a tensor\n",
    "        # Similar to np.mean but along a chosen axis\n",
    "        neigh_means = tf.reduce_mean(neigh_vecs, axis=1)\n",
    "       \n",
    "        # [nodes] x [out_dim]\n",
    "        from_neighs = tf.matmul(neigh_means, self.vars['neigh_weights'])\n",
    "        from_self = tf.matmul(self_vecs, self.vars[\"self_weights\"])\n",
    "         \n",
    "        if not self.concat:\n",
    "            # Returns the element-wise sum of a list of tensors\n",
    "            output = tf.add_n([from_self, from_neighs])\n",
    "        else:\n",
    "            output = tf.concat([from_self, from_neighs], axis=1)\n",
    "\n",
    "        # bias\n",
    "        if self.bias:\n",
    "            output += self.vars['bias']\n",
    "       \n",
    "        return self.act(output)\n",
    "\n",
    "# --------------------------------\n",
    "\n",
    "class MaxPoolingAggregator(Layer):\n",
    "    \n",
    "    \"\"\"\n",
    "    Aggregates via max-pooling over MLP functions.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim, model_size=\"small\", neigh_input_dim=None,\n",
    "            dropout=0., bias=False, act=tf.nn.relu, name=None, concat=False, **kwargs):\n",
    "        super(MaxPoolingAggregator, self).__init__(**kwargs)\n",
    "\n",
    "        self.dropout = dropout\n",
    "        self.bias = bias\n",
    "        self.act = act\n",
    "        self.concat = concat\n",
    "\n",
    "        if neigh_input_dim is None:\n",
    "            neigh_input_dim = input_dim\n",
    "\n",
    "        if name is not None:\n",
    "            name = '/' + name\n",
    "        else:\n",
    "            name = ''\n",
    "        \n",
    "        # Determine numnber of hidden neurons\n",
    "        if model_size == \"small\":\n",
    "            hidden_dim = self.hidden_dim = 512\n",
    "        elif model_size == \"big\":\n",
    "            hidden_dim = self.hidden_dim = 1024\n",
    "\n",
    "        self.mlp_layers = []\n",
    "        self.mlp_layers.append(Dense(input_dim=neigh_input_dim,\n",
    "                                 output_dim=hidden_dim,\n",
    "                                 act=tf.nn.relu,\n",
    "                                 dropout=dropout,\n",
    "                                 sparse_inputs=False,\n",
    "                                 logging=self.logging))\n",
    "\n",
    "        with tf.variable_scope(self.name + name + '_vars'):\n",
    "            self.vars['neigh_weights'] = glorot([hidden_dim, output_dim],\n",
    "                                                        name='neigh_weights')\n",
    "           \n",
    "            self.vars['self_weights'] = glorot([input_dim, output_dim],\n",
    "                                                        name='self_weights')\n",
    "            if self.bias:\n",
    "                self.vars['bias'] = zeros([self.output_dim], name='bias')\n",
    "\n",
    "        if self.logging:\n",
    "            self._log_vars()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.neigh_input_dim = neigh_input_dim\n",
    "\n",
    "    def _call(self, inputs):\n",
    "        self_vecs, neigh_vecs = inputs\n",
    "        neigh_h = neigh_vecs\n",
    "    \n",
    "        # Reshape data to fit layers\n",
    "        dims = tf.shape(neigh_h)\n",
    "        batch_size = dims[0]\n",
    "        num_neighbors = dims[1]\n",
    "        # [nodes * sampled neighbors] x [hidden_dim]\n",
    "        h_reshaped = tf.reshape(neigh_h, (batch_size * num_neighbors, self.neigh_input_dim))\n",
    "\n",
    "        # Run inputs through MLP layers\n",
    "        for l in self.mlp_layers:\n",
    "            h_reshaped = l(h_reshaped)\n",
    "        neigh_h = tf.reshape(h_reshaped, (batch_size, num_neighbors, self.hidden_dim))\n",
    "        \n",
    "        # Get the maximum of elements across dimensions of a tensor\n",
    "        neigh_h = tf.reduce_max(neigh_h, axis=1)\n",
    "        \n",
    "        from_neighs = tf.matmul(neigh_h, self.vars['neigh_weights'])\n",
    "        from_self = tf.matmul(self_vecs, self.vars[\"self_weights\"])\n",
    "        \n",
    "        if not self.concat:\n",
    "            output = tf.add_n([from_self, from_neighs])\n",
    "        else:\n",
    "            output = tf.concat([from_self, from_neighs], axis=1)\n",
    "\n",
    "        # bias\n",
    "        if self.bias:\n",
    "            output += self.vars['bias']\n",
    "       \n",
    "        return self.act(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b0b74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "from graphsage.layers import Layer\n",
    "\n",
    "import tensorflow as tf\n",
    "flags = tf.app.flags\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Classes that are used to sample node neighborhoods\n",
    "\"\"\"\n",
    "\n",
    "class UniformNeighborSampler(Layer):\n",
    "    \"\"\"\n",
    "    Uniformly samples neighbors.\n",
    "    Assumes that adj lists are padded with random re-sampling\n",
    "    \"\"\"\n",
    "    def __init__(self, adj_info, **kwargs):\n",
    "        super(UniformNeighborSampler, self).__init__(**kwargs)\n",
    "        self.adj_info = adj_info\n",
    "\n",
    "    def _call(self, inputs):\n",
    "        ids, num_samples = inputs\n",
    "        adj_lists = tf.nn.embedding_lookup(self.adj_info, ids) \n",
    "        adj_lists = tf.transpose(tf.random_shuffle(tf.transpose(adj_lists)))\n",
    "        adj_lists = tf.slice(adj_lists, [0,0], [-1, num_samples])\n",
    "        return adj_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af30659",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "from graphsage.inits import zeros\n",
    "from graphsage.layers import Layer\n",
    "import tensorflow as tf\n",
    "\n",
    "flags = tf.app.flags\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "\n",
    "class BipartiteEdgePredLayer(Layer):\n",
    "    def __init__(self, input_dim1, input_dim2, placeholders, dropout=False, act=tf.nn.sigmoid,\n",
    "            loss_fn='xent', neg_sample_weights=1.0,\n",
    "            bias=False, bilinear_weights=False, **kwargs):\n",
    "        \"\"\"\n",
    "        Basic class that applies skip-gram-like loss\n",
    "        (i.e., dot product of node+target and node and negative samples)\n",
    "        Args:\n",
    "            bilinear_weights: use a bilinear weight for affinity calculation: u^T A v. If set to\n",
    "                false, it is assumed that input dimensions are the same and the affinity will be \n",
    "                based on dot product.\n",
    "        \"\"\"\n",
    "        super(BipartiteEdgePredLayer, self).__init__(**kwargs)\n",
    "        self.input_dim1 = input_dim1\n",
    "        self.input_dim2 = input_dim2\n",
    "        self.act = act\n",
    "        self.bias = bias\n",
    "        self.eps = 1e-7\n",
    "\n",
    "        # Margin for hinge loss\n",
    "        self.margin = 0.1\n",
    "        self.neg_sample_weights = neg_sample_weights\n",
    "\n",
    "        self.bilinear_weights = bilinear_weights\n",
    "\n",
    "        if dropout:\n",
    "            self.dropout = placeholders['dropout']\n",
    "        else:\n",
    "            self.dropout = 0.\n",
    "\n",
    "        # output a likelihood term\n",
    "        self.output_dim = 1\n",
    "        with tf.variable_scope(self.name + '_vars'):\n",
    "            # bilinear form\n",
    "            if bilinear_weights:\n",
    "                #self.vars['weights'] = glorot([input_dim1, input_dim2],\n",
    "                #                              name='pred_weights')\n",
    "                self.vars['weights'] = tf.get_variable(\n",
    "                        'pred_weights', \n",
    "                        shape=(input_dim1, input_dim2),\n",
    "                        dtype=tf.float32, \n",
    "                        initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "            if self.bias:\n",
    "                self.vars['bias'] = zeros([self.output_dim], name='bias')\n",
    "\n",
    "        if loss_fn == 'xent':\n",
    "            self.loss_fn = self._xent_loss\n",
    "        elif loss_fn == 'skipgram':\n",
    "            self.loss_fn = self._skipgram_loss\n",
    "        elif loss_fn == 'hinge':\n",
    "            self.loss_fn = self._hinge_loss\n",
    "\n",
    "        if self.logging:\n",
    "            self._log_vars()\n",
    "\n",
    "    def affinity(self, inputs1, inputs2):\n",
    "        \"\"\" Affinity score between batch of inputs1 and inputs2.\n",
    "        Args:\n",
    "            inputs1: tensor of shape [batch_size x feature_size].\n",
    "        \"\"\"\n",
    "        # shape: [batch_size, input_dim1]\n",
    "        if self.bilinear_weights:\n",
    "            prod = tf.matmul(inputs2, tf.transpose(self.vars['weights']))\n",
    "            self.prod = prod\n",
    "            result = tf.reduce_sum(inputs1 * prod, axis=1)\n",
    "        else:\n",
    "            result = tf.reduce_sum(inputs1 * inputs2, axis=1)\n",
    "        return result\n",
    "\n",
    "    def neg_cost(self, inputs1, neg_samples, hard_neg_samples=None):\n",
    "        \"\"\" For each input in batch, compute the sum of its affinity to negative samples.\n",
    "\n",
    "        Returns:\n",
    "            Tensor of shape [batch_size x num_neg_samples]. For each node, a list of affinities to\n",
    "                negative samples is computed.\n",
    "        \"\"\"\n",
    "        if self.bilinear_weights:\n",
    "            inputs1 = tf.matmul(inputs1, self.vars['weights'])\n",
    "        neg_aff = tf.matmul(inputs1, tf.transpose(neg_samples))\n",
    "        return neg_aff\n",
    "\n",
    "    def loss(self, inputs1, inputs2, neg_samples):\n",
    "        \"\"\" negative sampling loss.\n",
    "        Args:\n",
    "            neg_samples: tensor of shape [num_neg_samples x input_dim2]. Negative samples for all\n",
    "            inputs in batch inputs1.\n",
    "        \"\"\"\n",
    "        return self.loss_fn(inputs1, inputs2, neg_samples)\n",
    "\n",
    "    def _xent_loss(self, inputs1, inputs2, neg_samples, hard_neg_samples=None):\n",
    "        aff = self.affinity(inputs1, inputs2)\n",
    "        neg_aff = self.neg_cost(inputs1, neg_samples, hard_neg_samples)\n",
    "        true_xent = tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "                labels=tf.ones_like(aff), logits=aff)\n",
    "        negative_xent = tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "                labels=tf.zeros_like(neg_aff), logits=neg_aff)\n",
    "        loss = tf.reduce_sum(true_xent) + self.neg_sample_weights * tf.reduce_sum(negative_xent)\n",
    "        return loss\n",
    "\n",
    "    def _skipgram_loss(self, inputs1, inputs2, neg_samples, hard_neg_samples=None):\n",
    "        aff = self.affinity(inputs1, inputs2)\n",
    "        neg_aff = self.neg_cost(inputs1, neg_samples, hard_neg_samples)\n",
    "        neg_cost = tf.log(tf.reduce_sum(tf.exp(neg_aff), axis=1))\n",
    "        loss = tf.reduce_sum(aff - neg_cost)\n",
    "        return loss\n",
    "\n",
    "    def _hinge_loss(self, inputs1, inputs2, neg_samples, hard_neg_samples=None):\n",
    "        aff = self.affinity(inputs1, inputs2)\n",
    "        neg_aff = self.neg_cost(inputs1, neg_samples, hard_neg_samples)\n",
    "        diff = tf.nn.relu(tf.subtract(neg_aff, tf.expand_dims(aff, 1) - self.margin), name='diff')\n",
    "        loss = tf.reduce_sum(diff)\n",
    "        self.neg_shape = tf.shape(neg_aff)\n",
    "        return loss\n",
    "\n",
    "    def weights_norm(self):\n",
    "        return tf.nn.l2_norm(self.vars['weights'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723db1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(object):\n",
    "    def __init__(self, **kwargs):\n",
    "        allowed_kwargs = {'name', 'logging', 'model_size'}\n",
    "        for kwarg in kwargs.keys():\n",
    "            assert kwarg in allowed_kwargs, 'Invalid keyword argument: ' + kwarg\n",
    "        name = kwargs.get('name')\n",
    "        if not name:\n",
    "            name = self.__class__.__name__.lower()\n",
    "        self.name = name\n",
    "\n",
    "        logging = kwargs.get('logging', False)\n",
    "        self.logging = logging\n",
    "\n",
    "        self.vars = {}\n",
    "        self.placeholders = {}\n",
    "\n",
    "        self.layers = []\n",
    "        self.activations = []\n",
    "\n",
    "        self.inputs = None\n",
    "        self.outputs = None\n",
    "\n",
    "        self.loss = 0\n",
    "        self.accuracy = 0\n",
    "        self.optimizer = None\n",
    "        self.opt_op = None\n",
    "\n",
    "    def _build(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def build(self):\n",
    "        \"\"\" Wrapper for _build() \"\"\"\n",
    "        with tf.variable_scope(self.name):\n",
    "            self._build()\n",
    "\n",
    "        # Build sequential layer model\n",
    "        self.activations.append(self.inputs)\n",
    "        for layer in self.layers:\n",
    "            hidden = layer(self.activations[-1])\n",
    "            self.activations.append(hidden)\n",
    "        self.outputs = self.activations[-1]\n",
    "\n",
    "        # Store model variables for easy access\n",
    "        variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=self.name)\n",
    "        self.vars = {var.name: var for var in variables}\n",
    "\n",
    "        # Build metrics\n",
    "        self._loss()\n",
    "        self._accuracy()\n",
    "\n",
    "        self.opt_op = self.optimizer.minimize(self.loss)\n",
    "\n",
    "    def predict(self):\n",
    "        pass\n",
    "\n",
    "    def _loss(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def _accuracy(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def save(self, sess=None):\n",
    "        if not sess:\n",
    "            raise AttributeError(\"TensorFlow session not provided.\")\n",
    "        saver = tf.train.Saver(self.vars)\n",
    "        save_path = saver.save(sess, \"tmp/%s.ckpt\" % self.name)\n",
    "        print(\"Model saved in file: %s\" % save_path)\n",
    "\n",
    "    def load(self, sess=None):\n",
    "        if not sess:\n",
    "            raise AttributeError(\"TensorFlow session not provided.\")\n",
    "        saver = tf.train.Saver(self.vars)\n",
    "        save_path = \"tmp/%s.ckpt\" % self.name\n",
    "        saver.restore(sess, save_path)\n",
    "        print(\"Model restored from file: %s\" % save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52278f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get model\n",
    "class GeneralizedModel(Model):\n",
    "    \"\"\"\n",
    "    Base class for models that aren't constructed from traditional, sequential layers.\n",
    "    Subclasses must set self.outputs in _build method\n",
    "\n",
    "    (Removes the layers idiom from build method of the Model class)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super(GeneralizedModel, self).__init__(**kwargs)\n",
    "        \n",
    "    def build(self):\n",
    "        \"\"\" Wrapper for _build() \"\"\"\n",
    "        with tf.variable_scope(self.name):\n",
    "            self._build()\n",
    "\n",
    "        # Store model variables for easy access\n",
    "        # Every element of the collection is filtered and its returned\n",
    "        # only if the name of the node starts with the specified scope\n",
    "        variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=self.name)\n",
    "        self.vars = {var.name: var for var in variables}\n",
    "\n",
    "        # Build metrics\n",
    "        self._loss()\n",
    "        self._accuracy()\n",
    "\n",
    "        self.opt_op = self.optimizer.minimize(self.loss)\n",
    "\n",
    "# SAGEInfo is a namedtuple that specifies the parameters \n",
    "# of the recursive GraphSAGE layers\n",
    "SAGEInfo = namedtuple(\"SAGEInfo\",\n",
    "    ['layer_name', # name of the layer (to get feature embedding etc.)\n",
    "     'neigh_sampler', # callable neigh_sampler constructor\n",
    "     'num_samples',\n",
    "     'output_dim' # the output (i.e., hidden) dimension\n",
    "    ])\n",
    "\n",
    "class SampleAndAggregate(GeneralizedModel):\n",
    "    \"\"\"\n",
    "    Base implementation of unsupervised GraphSAGE\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, placeholders, features, adj, degrees,\n",
    "            layer_infos, concat=True, aggregator_type=\"mean\", \n",
    "            model_size=\"small\", identity_dim=0,\n",
    "            **kwargs):\n",
    "        '''\n",
    "        Args:\n",
    "            - placeholders: Stanford TensorFlow placeholder object.\n",
    "            - features: Numpy array with node features. \n",
    "                        NOTE: Pass a None object to train in featureless mode (identity features for nodes)!\n",
    "            - adj: Numpy array with adjacency lists (padded with random re-samples)\n",
    "            - degrees: Numpy array with node degrees. \n",
    "            - layer_infos: List of SAGEInfo namedtuples that describe the parameters of all \n",
    "                   the recursive layers. See SAGEInfo definition above.\n",
    "            - concat: whether to concatenate during recursive iterations\n",
    "            - aggregator_type: how to aggregate neighbor information\n",
    "            - model_size: one of \"small\" and \"big\"\n",
    "            - identity_dim: Set to positive int to use identity features (slow and cannot generalize, but better accuracy)\n",
    "        '''\n",
    "        super(SampleAndAggregate, self).__init__(**kwargs)\n",
    "        if aggregator_type == \"mean\":\n",
    "            self.aggregator_cls = MeanAggregator\n",
    "        elif aggregator_type == \"seq\":\n",
    "            self.aggregator_cls = SeqAggregator\n",
    "        elif aggregator_type == \"maxpool\":\n",
    "            self.aggregator_cls = MaxPoolingAggregator\n",
    "        elif aggregator_type == \"meanpool\":\n",
    "            self.aggregator_cls = MeanPoolingAggregator\n",
    "        elif aggregator_type == \"gcn\":\n",
    "            self.aggregator_cls = GCNAggregator\n",
    "        else:\n",
    "            raise Exception(\"Unknown aggregator: \", self.aggregator_cls)\n",
    "\n",
    "        # get info from placeholders...\n",
    "        self.inputs1 = placeholders[\"batch1\"]\n",
    "        self.inputs2 = placeholders[\"batch2\"]\n",
    "        self.model_size = model_size\n",
    "        self.adj_info = adj\n",
    "        \n",
    "        if identity_dim > 0:\n",
    "            self.embeds = tf.get_variable(\"node_embeddings\", [adj.get_shape().as_list()[0], identity_dim])\n",
    "        else:\n",
    "            self.embeds = None\n",
    "        \n",
    "        if features is None: \n",
    "            if identity_dim == 0:\n",
    "                raise Exception(\"Must have a positive value for identity feature dimension if no input features given.\")\n",
    "            self.features = self.embeds\n",
    "        else:\n",
    "            self.features = tf.Variable(tf.constant(features, dtype=tf.float32), trainable=False)\n",
    "            if not self.embeds is None:\n",
    "                self.features = tf.concat([self.embeds, self.features], axis=1)\n",
    "        \n",
    "        '''\n",
    "        The differences between constants and variables are that when you declare some constant, its value \n",
    "        can't be changed in the future (also the initialization should be with a value, not with operation).\n",
    "\n",
    "        Nevertheless, when you declare a Variable, you can change its value in the future with tf.assign() method\n",
    "        (and the initialization can be achieved with a value or operation).\n",
    "        '''\n",
    "        \n",
    "        self.degrees = degrees\n",
    "        self.concat = concat\n",
    "\n",
    "        self.dims = [(0 if features is None else features.shape[1]) + identity_dim]\n",
    "        self.dims.extend([layer_infos[i].output_dim for i in range(len(layer_infos))])\n",
    "        self.batch_size = placeholders[\"batch_size\"]\n",
    "        self.placeholders = placeholders\n",
    "        self.layer_infos = layer_infos\n",
    "\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate=FLAGS.learning_rate)\n",
    "\n",
    "        self.build()\n",
    "    \n",
    "    # Returns the list of sampled neighbors (samples) and the list of support sizes (support_sizes)\n",
    "    def sample(self, inputs, layer_infos, batch_size=None):\n",
    "        \"\"\" Sample neighbors to be the supportive fields for multi-layer convolutions.\n",
    "\n",
    "        Args:\n",
    "            inputs: batch inputs\n",
    "            batch_size: the number of inputs (different for batch inputs and negative samples).\n",
    "        \"\"\"\n",
    "        \n",
    "        if batch_size is None:\n",
    "            batch_size = self.batch_size\n",
    "        samples = [inputs]\n",
    "        # size of convolution support at each layer per node\n",
    "        # size of the convolution support determines the number of neighbors\n",
    "        # that are considered for aggregation at each layer. (i.e., how many neighbours do I learn from?)\n",
    "        support_size = 1\n",
    "        support_sizes = [support_size]\n",
    "        for k in range(len(layer_infos)):\n",
    "            t = len(layer_infos) - k - 1\n",
    "            support_size *= layer_infos[t].num_samples\n",
    "            sampler = layer_infos[t].neigh_sampler\n",
    "            node = sampler((samples[k], layer_infos[t].num_samples))\n",
    "            samples.append(tf.reshape(node, [support_size * batch_size,]))\n",
    "            support_sizes.append(support_size)\n",
    "        return samples, support_sizes\n",
    "\n",
    "    # Returns the hidden representations at the final layer and the list of aggregators used in the process\n",
    "    def aggregate(self, samples, input_features, dims, num_samples, support_sizes, batch_size=None,\n",
    "            aggregators=None, name=None, concat=False, model_size=\"small\"):\n",
    "        \"\"\" At each layer, aggregate hidden representations of neighbors to compute the hidden representations \n",
    "            at next layer.\n",
    "        Args:\n",
    "            samples: a list of samples of variable hops away for convolving at each layer of the\n",
    "                network. Length is the number of layers + 1. Each is a vector of node indices.\n",
    "            input_features: the input features for each sample of various hops away.\n",
    "            dims: a list of dimensions of the hidden representations from the input layer to the\n",
    "                final layer. Length is the number of layers + 1.\n",
    "            num_samples: list of number of samples for each layer.\n",
    "            support_sizes: the number of nodes to gather information from for each layer.\n",
    "            batch_size: the number of inputs (different for batch inputs and negative samples).\n",
    "        Returns:\n",
    "            The hidden representation at the final layer for all nodes in batch\n",
    "        \"\"\"\n",
    "\n",
    "        if batch_size is None:\n",
    "            batch_size = self.batch_size\n",
    "\n",
    "        hidden = [tf.nn.embedding_lookup(input_features, node_samples) for node_samples in samples]\n",
    "        new_agg = aggregators is None\n",
    "        if new_agg:\n",
    "            aggregators = []\n",
    "            \n",
    "        # Iterate through each sample in each num_sample to get their representation from their neighbours\n",
    "        for layer in range(len(num_samples)):\n",
    "            if new_agg:\n",
    "                dim_mult = 2 if concat and (layer != 0) else 1\n",
    "                # aggregator at current layer\n",
    "                if layer == len(num_samples) - 1:\n",
    "                    aggregator = self.aggregator_cls(dim_mult*dims[layer], dims[layer+1], act=lambda x : x,\n",
    "                            dropout=self.placeholders['dropout'], \n",
    "                            name=name, concat=concat, model_size=model_size)\n",
    "                else:\n",
    "     '''               \n",
    "    def __init__(self, input_dim, output_dim, neigh_input_dim=None,\n",
    "            dropout=0., bias=False, act=tf.nn.relu, \n",
    "            name=None, concat=False, **kwargs):\n",
    "     '''\n",
    "                    aggregator = self.aggregator_cls(dim_mult*dims[layer], dims[layer+1],\n",
    "                            dropout=self.placeholders['dropout'], \n",
    "                            name=name, concat=concat, model_size=model_size)\n",
    "                aggregators.append(aggregator)\n",
    "            else:\n",
    "                aggregator = aggregators[layer]\n",
    "            # hidden representation at current layer for all support nodes that are various hops away\n",
    "            next_hidden = []\n",
    "            # as layer increases, the number of support nodes needed decreases\n",
    "            \n",
    "            # Hop from base node to further nodes\n",
    "            for hop in range(len(num_samples) - layer):\n",
    "                dim_mult = 2 if concat and (layer != 0) else 1\n",
    "                neigh_dims = [batch_size * support_sizes[hop], \n",
    "                              num_samples[len(num_samples) - hop - 1], \n",
    "                              dim_mult*dims[layer]]\n",
    "                h = aggregator((hidden[hop],\n",
    "                                tf.reshape(hidden[hop + 1], neigh_dims)))\n",
    "                next_hidden.append(h)\n",
    "            hidden = next_hidden\n",
    "        return hidden[0], aggregators\n",
    "\n",
    "    def _build(self):\n",
    "        labels = tf.reshape(\n",
    "                tf.cast(self.placeholders['batch2'], dtype=tf.int64),\n",
    "                [self.batch_size, 1])\n",
    "        self.neg_samples, _, _ = (tf.nn.fixed_unigram_candidate_sampler(\n",
    "            true_classes=labels,\n",
    "            num_true=1,\n",
    "            num_sampled=FLAGS.neg_sample_size,\n",
    "            unique=False,\n",
    "            range_max=len(self.degrees),\n",
    "            distortion=0.75,\n",
    "            unigrams=self.degrees.tolist()))\n",
    "\n",
    "           \n",
    "        # perform \"convolution\"\n",
    "        samples1, support_sizes1 = self.sample(self.inputs1, self.layer_infos)\n",
    "        samples2, support_sizes2 = self.sample(self.inputs2, self.layer_infos)\n",
    "        num_samples = [layer_info.num_samples for layer_info in self.layer_infos]\n",
    "        self.outputs1, self.aggregators = self.aggregate(samples1, [self.features], self.dims, num_samples,\n",
    "                support_sizes1, concat=self.concat, model_size=self.model_size)\n",
    "        self.outputs2, _ = self.aggregate(samples2, [self.features], self.dims, num_samples,\n",
    "                support_sizes2, aggregators=self.aggregators, concat=self.concat,\n",
    "                model_size=self.model_size)\n",
    "\n",
    "        neg_samples, neg_support_sizes = self.sample(self.neg_samples, self.layer_infos,\n",
    "            FLAGS.neg_sample_size)\n",
    "        self.neg_outputs, _ = self.aggregate(neg_samples, [self.features], self.dims, num_samples,\n",
    "                neg_support_sizes, batch_size=FLAGS.neg_sample_size, aggregators=self.aggregators,\n",
    "                concat=self.concat, model_size=self.model_size)\n",
    "\n",
    "        dim_mult = 2 if self.concat else 1\n",
    "        self.link_pred_layer = BipartiteEdgePredLayer(dim_mult*self.dims[-1],\n",
    "                dim_mult*self.dims[-1], self.placeholders, act=tf.nn.sigmoid, \n",
    "                bilinear_weights=False,\n",
    "                name='edge_predict')\n",
    "\n",
    "        self.outputs1 = tf.nn.l2_normalize(self.outputs1, 1)\n",
    "        self.outputs2 = tf.nn.l2_normalize(self.outputs2, 1)\n",
    "        self.neg_outputs = tf.nn.l2_normalize(self.neg_outputs, 1)\n",
    "\n",
    "    def build(self):\n",
    "        self._build()\n",
    "\n",
    "        # TF graph management\n",
    "        self._loss()\n",
    "        self._accuracy()\n",
    "        self.loss = self.loss / tf.cast(self.batch_size, tf.float32)\n",
    "        grads_and_vars = self.optimizer.compute_gradients(self.loss)\n",
    "        clipped_grads_and_vars = [(tf.clip_by_value(grad, -5.0, 5.0) if grad is not None else None, var) \n",
    "                for grad, var in grads_and_vars]\n",
    "        self.grad, _ = clipped_grads_and_vars[0]\n",
    "        \n",
    "        # Tensorflow 2.0: Finally, instead of assigning the optimizer operation to self.opt_op,\n",
    "        # you can directly call self.optimizer.apply_gradients(clipped_grads_and_vars) to update\n",
    "        # the model's trainable variables based on the computed gradients.\n",
    "        self.opt_op = self.optimizer.apply_gradients(clipped_grads_and_vars)\n",
    "\n",
    "    def _loss(self):\n",
    "        for aggregator in self.aggregators:\n",
    "            for var in aggregator.vars.values():\n",
    "                self.loss += FLAGS.weight_decay * tf.nn.l2_loss(var)\n",
    "          \n",
    "        self.loss += self.link_pred_layer.loss(self.outputs1, self.outputs2, self.neg_outputs) \n",
    "        tf.summary.scalar('loss', self.loss)\n",
    "\n",
    "    def _accuracy(self):\n",
    "        # shape: [batch_size]\n",
    "        aff = self.link_pred_layer.affinity(self.outputs1, self.outputs2)\n",
    "        # shape : [batch_size x num_neg_samples]\n",
    "        self.neg_aff = self.link_pred_layer.neg_cost(self.outputs1, self.neg_outputs)\n",
    "        self.neg_aff = tf.reshape(self.neg_aff, [self.batch_size, FLAGS.neg_sample_size])\n",
    "        _aff = tf.expand_dims(aff, axis=1)\n",
    "        self.aff_all = tf.concat(axis=1, values=[self.neg_aff, _aff])\n",
    "        size = tf.shape(self.aff_all)[1]\n",
    "        _, indices_of_ranks = tf.nn.top_k(self.aff_all, k=size)\n",
    "        _, self.ranks = tf.nn.top_k(-indices_of_ranks, k=size)\n",
    "        self.mrr = tf.reduce_mean(tf.div(1.0, tf.cast(self.ranks[:, -1] + 1, tf.float32)))\n",
    "        tf.summary.scalar('mrr', self.mrr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19439883",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get mini batch iterator\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(123)\n",
    "\n",
    "class EdgeMinibatchIterator(object):\n",
    "    \n",
    "    \"\"\" This minibatch iterator iterates over batches of sampled edges or\n",
    "    random pairs of co-occuring edges.\n",
    "\n",
    "    G -- networkx graph\n",
    "    id2idx -- dict mapping node ids to index in feature tensor\n",
    "    placeholders -- tensorflow placeholders object\n",
    "    context_pairs -- if not none, then a list of co-occuring node pairs (from random walks)\n",
    "    batch_size -- size of the minibatches\n",
    "    max_degree -- maximum size of the downsampled adjacency lists\n",
    "    n2v_retrain -- signals that the iterator is being used to add new embeddings to a n2v model\n",
    "    fixed_n2v -- signals that the iterator is being used to retrain n2v with only existing nodes as context\n",
    "    \"\"\"\n",
    "    def __init__(self, G, id2idx, \n",
    "            placeholders, context_pairs=None, batch_size=100, max_degree=25,\n",
    "            n2v_retrain=False, fixed_n2v=False,\n",
    "            **kwargs):\n",
    "\n",
    "        self.G = G\n",
    "        self.nodes = G.nodes()\n",
    "        self.id2idx = id2idx\n",
    "        self.placeholders = placeholders\n",
    "        self.batch_size = batch_size\n",
    "        self.max_degree = max_degree\n",
    "        self.batch_num = 0\n",
    "\n",
    "        self.nodes = np.random.permutation(G.nodes())\n",
    "        self.adj, self.deg = self.construct_adj()\n",
    "        self.test_adj = self.construct_test_adj()\n",
    "        if context_pairs is None:\n",
    "            edges = G.edges()\n",
    "        else:\n",
    "            edges = context_pairs\n",
    "        self.train_edges = self.edges = np.random.permutation(edges)\n",
    "        if not n2v_retrain:\n",
    "            self.train_edges = self._remove_isolated(self.train_edges)\n",
    "            self.val_edges = [e for e in G.edges() if G[e[0]][e[1]]['train_removed']]\n",
    "        else:\n",
    "            if fixed_n2v:\n",
    "                self.train_edges = self.val_edges = self._n2v_prune(self.edges)\n",
    "            else:\n",
    "                self.train_edges = self.val_edges = self.edges\n",
    "\n",
    "        print(len([n for n in G.nodes() if not G.node[n]['test'] and not G.node[n]['val']]), 'train nodes')\n",
    "        print(len([n for n in G.nodes() if G.node[n]['test'] or G.node[n]['val']]), 'test nodes')\n",
    "        self.val_set_size = len(self.val_edges)\n",
    "\n",
    "    def _n2v_prune(self, edges):\n",
    "        is_val = lambda n : self.G.node[n][\"val\"] or self.G.node[n][\"test\"]\n",
    "        return [e for e in edges if not is_val(e[1])]\n",
    "\n",
    "    def _remove_isolated(self, edge_list):\n",
    "        new_edge_list = []\n",
    "        missing = 0\n",
    "        for n1, n2 in edge_list:\n",
    "            if not n1 in self.G.node or not n2 in self.G.node:\n",
    "                missing += 1\n",
    "                continue\n",
    "            if (self.deg[self.id2idx[n1]] == 0 or self.deg[self.id2idx[n2]] == 0) \\\n",
    "                    and (not self.G.node[n1]['test'] or self.G.node[n1]['val']) \\\n",
    "                    and (not self.G.node[n2]['test'] or self.G.node[n2]['val']):\n",
    "                continue\n",
    "            else:\n",
    "                new_edge_list.append((n1,n2))\n",
    "        print(\"Unexpected missing:\", missing)\n",
    "        return new_edge_list\n",
    "\n",
    "    def construct_adj(self):\n",
    "        adj = len(self.id2idx)*np.ones((len(self.id2idx)+1, self.max_degree))\n",
    "        deg = np.zeros((len(self.id2idx),))\n",
    "\n",
    "        for nodeid in self.G.nodes():\n",
    "            if self.G.node[nodeid]['test'] or self.G.node[nodeid]['val']:\n",
    "                continue\n",
    "            neighbors = np.array([self.id2idx[neighbor] \n",
    "                for neighbor in self.G.neighbors(nodeid)\n",
    "                if (not self.G[nodeid][neighbor]['train_removed'])])\n",
    "            deg[self.id2idx[nodeid]] = len(neighbors)\n",
    "            if len(neighbors) == 0:\n",
    "                continue\n",
    "            if len(neighbors) > self.max_degree:\n",
    "                neighbors = np.random.choice(neighbors, self.max_degree, replace=False)\n",
    "            elif len(neighbors) < self.max_degree:\n",
    "                neighbors = np.random.choice(neighbors, self.max_degree, replace=True)\n",
    "            adj[self.id2idx[nodeid], :] = neighbors\n",
    "        return adj, deg\n",
    "\n",
    "    def construct_test_adj(self):\n",
    "        adj = len(self.id2idx)*np.ones((len(self.id2idx)+1, self.max_degree))\n",
    "        for nodeid in self.G.nodes():\n",
    "            neighbors = np.array([self.id2idx[neighbor] \n",
    "                for neighbor in self.G.neighbors(nodeid)])\n",
    "            if len(neighbors) == 0:\n",
    "                continue\n",
    "            if len(neighbors) > self.max_degree:\n",
    "                neighbors = np.random.choice(neighbors, self.max_degree, replace=False)\n",
    "            elif len(neighbors) < self.max_degree:\n",
    "                neighbors = np.random.choice(neighbors, self.max_degree, replace=True)\n",
    "            adj[self.id2idx[nodeid], :] = neighbors\n",
    "        return adj\n",
    "\n",
    "    def end(self):\n",
    "        return self.batch_num * self.batch_size >= len(self.train_edges)\n",
    "\n",
    "    def batch_feed_dict(self, batch_edges):\n",
    "        batch1 = []\n",
    "        batch2 = []\n",
    "        for node1, node2 in batch_edges:\n",
    "            batch1.append(self.id2idx[node1])\n",
    "            batch2.append(self.id2idx[node2])\n",
    "\n",
    "        feed_dict = dict()\n",
    "        feed_dict.update({self.placeholders['batch_size'] : len(batch_edges)})\n",
    "        feed_dict.update({self.placeholders['batch1']: batch1})\n",
    "        feed_dict.update({self.placeholders['batch2']: batch2})\n",
    "\n",
    "        return feed_dict\n",
    "\n",
    "    def next_minibatch_feed_dict(self):\n",
    "        start_idx = self.batch_num * self.batch_size\n",
    "        self.batch_num += 1\n",
    "        end_idx = min(start_idx + self.batch_size, len(self.train_edges))\n",
    "        batch_edges = self.train_edges[start_idx : end_idx]\n",
    "        return self.batch_feed_dict(batch_edges)\n",
    "\n",
    "    def num_training_batches(self):\n",
    "        return len(self.train_edges) // self.batch_size + 1\n",
    "\n",
    "    def val_feed_dict(self, size=None):\n",
    "        edge_list = self.val_edges\n",
    "        if size is None:\n",
    "            return self.batch_feed_dict(edge_list)\n",
    "        else:\n",
    "            ind = np.random.permutation(len(edge_list))\n",
    "            val_edges = [edge_list[i] for i in ind[:min(size, len(ind))]]\n",
    "            return self.batch_feed_dict(val_edges)\n",
    "\n",
    "    def incremental_val_feed_dict(self, size, iter_num):\n",
    "        edge_list = self.val_edges\n",
    "        val_edges = edge_list[iter_num*size:min((iter_num+1)*size, \n",
    "            len(edge_list))]\n",
    "        return self.batch_feed_dict(val_edges), (iter_num+1)*size >= len(self.val_edges), val_edges\n",
    "\n",
    "    def incremental_embed_feed_dict(self, size, iter_num):\n",
    "        node_list = self.nodes\n",
    "        val_nodes = node_list[iter_num*size:min((iter_num+1)*size, \n",
    "            len(node_list))]\n",
    "        val_edges = [(n,n) for n in val_nodes]\n",
    "        return self.batch_feed_dict(val_edges), (iter_num+1)*size >= len(node_list), val_edges\n",
    "\n",
    "    def label_val(self):\n",
    "        train_edges = []\n",
    "        val_edges = []\n",
    "        for n1, n2 in self.G.edges():\n",
    "            if (self.G.node[n1]['val'] or self.G.node[n1]['test'] \n",
    "                    or self.G.node[n2]['val'] or self.G.node[n2]['test']):\n",
    "                val_edges.append((n1,n2))\n",
    "            else:\n",
    "                train_edges.append((n1,n2))\n",
    "        return train_edges, val_edges\n",
    "\n",
    "    def shuffle(self):\n",
    "        \"\"\" Re-shuffle the training set.\n",
    "            Also reset the batch number.\n",
    "        \"\"\"\n",
    "        self.train_edges = np.random.permutation(self.train_edges)\n",
    "        self.nodes = np.random.permutation(self.nodes)\n",
    "        self.batch_num = 0\n",
    "\n",
    "class NodeMinibatchIterator(object):\n",
    "    \n",
    "    \"\"\" \n",
    "    This minibatch iterator iterates over nodes for supervised learning.\n",
    "\n",
    "    G -- networkx graph\n",
    "    id2idx -- dict mapping node ids to integer values indexing feature tensor\n",
    "    placeholders -- standard tensorflow placeholders object for feeding\n",
    "    label_map -- map from node ids to class values (integer or list)\n",
    "    num_classes -- number of output classes\n",
    "    batch_size -- size of the minibatches\n",
    "    max_degree -- maximum size of the downsampled adjacency lists\n",
    "    \"\"\"\n",
    "    def __init__(self, G, id2idx, \n",
    "            placeholders, label_map, num_classes, \n",
    "            batch_size=100, max_degree=25,\n",
    "            **kwargs):\n",
    "\n",
    "        self.G = G\n",
    "        self.nodes = G.nodes()\n",
    "        self.id2idx = id2idx\n",
    "        self.placeholders = placeholders\n",
    "        self.batch_size = batch_size\n",
    "        self.max_degree = max_degree\n",
    "        self.batch_num = 0\n",
    "        self.label_map = label_map\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.adj, self.deg = self.construct_adj()\n",
    "        self.test_adj = self.construct_test_adj()\n",
    "\n",
    "        self.val_nodes = [n for n in self.G.nodes() if self.G.node[n]['val']]\n",
    "        self.test_nodes = [n for n in self.G.nodes() if self.G.node[n]['test']]\n",
    "\n",
    "        self.no_train_nodes_set = set(self.val_nodes + self.test_nodes)\n",
    "        self.train_nodes = set(G.nodes()).difference(self.no_train_nodes_set)\n",
    "        # don't train on nodes that only have edges to test set\n",
    "        self.train_nodes = [n for n in self.train_nodes if self.deg[id2idx[n]] > 0]\n",
    "\n",
    "    def _make_label_vec(self, node):\n",
    "        label = self.label_map[node]\n",
    "        if isinstance(label, list):\n",
    "            label_vec = np.array(label)\n",
    "        else:\n",
    "            label_vec = np.zeros((self.num_classes))\n",
    "            class_ind = self.label_map[node]\n",
    "            label_vec[class_ind] = 1\n",
    "        return label_vec\n",
    "\n",
    "    def construct_adj(self):\n",
    "        adj = len(self.id2idx)*np.ones((len(self.id2idx)+1, self.max_degree))\n",
    "        deg = np.zeros((len(self.id2idx),))\n",
    "\n",
    "        for nodeid in self.G.nodes():\n",
    "            if self.G.node[nodeid]['test'] or self.G.node[nodeid]['val']:\n",
    "                continue\n",
    "            neighbors = np.array([self.id2idx[neighbor] \n",
    "                for neighbor in self.G.neighbors(nodeid)\n",
    "                if (not self.G[nodeid][neighbor]['train_removed'])])\n",
    "            deg[self.id2idx[nodeid]] = len(neighbors)\n",
    "            if len(neighbors) == 0:\n",
    "                continue\n",
    "            if len(neighbors) > self.max_degree:\n",
    "                neighbors = np.random.choice(neighbors, self.max_degree, replace=False)\n",
    "            elif len(neighbors) < self.max_degree:\n",
    "                neighbors = np.random.choice(neighbors, self.max_degree, replace=True)\n",
    "            adj[self.id2idx[nodeid], :] = neighbors\n",
    "        return adj, deg\n",
    "\n",
    "    def construct_test_adj(self):\n",
    "        adj = len(self.id2idx)*np.ones((len(self.id2idx)+1, self.max_degree))\n",
    "        for nodeid in self.G.nodes():\n",
    "            neighbors = np.array([self.id2idx[neighbor] \n",
    "                for neighbor in self.G.neighbors(nodeid)])\n",
    "            if len(neighbors) == 0:\n",
    "                continue\n",
    "            if len(neighbors) > self.max_degree:\n",
    "                neighbors = np.random.choice(neighbors, self.max_degree, replace=False)\n",
    "            elif len(neighbors) < self.max_degree:\n",
    "                neighbors = np.random.choice(neighbors, self.max_degree, replace=True)\n",
    "            adj[self.id2idx[nodeid], :] = neighbors\n",
    "        return adj\n",
    "\n",
    "    def end(self):\n",
    "        return self.batch_num * self.batch_size >= len(self.train_nodes)\n",
    "\n",
    "    def batch_feed_dict(self, batch_nodes, val=False):\n",
    "        batch1id = batch_nodes\n",
    "        batch1 = [self.id2idx[n] for n in batch1id]\n",
    "              \n",
    "        labels = np.vstack([self._make_label_vec(node) for node in batch1id])\n",
    "        feed_dict = dict()\n",
    "        feed_dict.update({self.placeholders['batch_size'] : len(batch1)})\n",
    "        feed_dict.update({self.placeholders['batch']: batch1})\n",
    "        feed_dict.update({self.placeholders['labels']: labels})\n",
    "\n",
    "        return feed_dict, labels\n",
    "\n",
    "    def node_val_feed_dict(self, size=None, test=False):\n",
    "        if test:\n",
    "            val_nodes = self.test_nodes\n",
    "        else:\n",
    "            val_nodes = self.val_nodes\n",
    "        if not size is None:\n",
    "            val_nodes = np.random.choice(val_nodes, size, replace=True)\n",
    "        # add a dummy neighbor\n",
    "        ret_val = self.batch_feed_dict(val_nodes)\n",
    "        return ret_val[0], ret_val[1]\n",
    "\n",
    "    def incremental_node_val_feed_dict(self, size, iter_num, test=False):\n",
    "        if test:\n",
    "            val_nodes = self.test_nodes\n",
    "        else:\n",
    "            val_nodes = self.val_nodes\n",
    "        val_node_subset = val_nodes[iter_num*size:min((iter_num+1)*size, \n",
    "            len(val_nodes))]\n",
    "\n",
    "        # add a dummy neighbor\n",
    "        ret_val = self.batch_feed_dict(val_node_subset)\n",
    "        return ret_val[0], ret_val[1], (iter_num+1)*size >= len(val_nodes), val_node_subset\n",
    "\n",
    "    def num_training_batches(self):\n",
    "        return len(self.train_nodes) // self.batch_size + 1\n",
    "\n",
    "    def next_minibatch_feed_dict(self):\n",
    "        start_idx = self.batch_num * self.batch_size\n",
    "        self.batch_num += 1\n",
    "        end_idx = min(start_idx + self.batch_size, len(self.train_nodes))\n",
    "        batch_nodes = self.train_nodes[start_idx : end_idx]\n",
    "        return self.batch_feed_dict(batch_nodes)\n",
    "\n",
    "    def incremental_embed_feed_dict(self, size, iter_num):\n",
    "        node_list = self.nodes\n",
    "        val_nodes = node_list[iter_num*size:min((iter_num+1)*size, \n",
    "            len(node_list))]\n",
    "        return self.batch_feed_dict(val_nodes), (iter_num+1)*size >= len(node_list), val_nodes\n",
    "\n",
    "    def shuffle(self):\n",
    "        \"\"\" Re-shuffle the training set.\n",
    "            Also reset the batch number.\n",
    "        \"\"\"\n",
    "        self.train_nodes = np.random.permutation(self.train_nodes)\n",
    "        self.batch_num = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b1d78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get model for unsupervised train\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from graphsage.models import SampleAndAggregate, SAGEInfo, Node2VecModel\n",
    "from graphsage.minibatch import EdgeMinibatchIterator\n",
    "from graphsage.neigh_samplers import UniformNeighborSampler\n",
    "from graphsage.utils import load_data\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "\n",
    "# Set random seed\n",
    "seed = 123\n",
    "np.random.seed(seed)\n",
    "tf.set_random_seed(seed)\n",
    "\n",
    "# Settings\n",
    "flags = tf.app.flags\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "tf.app.flags.DEFINE_boolean('log_device_placement', False,\n",
    "                            \"\"\"Whether to log device placement.\"\"\")\n",
    "#core params..\n",
    "flags.DEFINE_string('model', 'graphsage', 'model names. See README for possible values.')  \n",
    "flags.DEFINE_float('learning_rate', 0.00001, 'initial learning rate.')\n",
    "flags.DEFINE_string(\"model_size\", \"small\", \"Can be big or small; model specific def'ns\")\n",
    "flags.DEFINE_string('train_prefix', '', 'name of the object file that stores the training data. must be specified.')\n",
    "\n",
    "# left to default values in main experiments \n",
    "flags.DEFINE_integer('epochs', 1, 'number of epochs to train.')\n",
    "flags.DEFINE_float('dropout', 0.0, 'dropout rate (1 - keep probability).')\n",
    "flags.DEFINE_float('weight_decay', 0.0, 'weight for l2 loss on embedding matrix.')\n",
    "flags.DEFINE_integer('max_degree', 100, 'maximum node degree.')\n",
    "flags.DEFINE_integer('samples_1', 25, 'number of samples in layer 1')\n",
    "flags.DEFINE_integer('samples_2', 10, 'number of users samples in layer 2')\n",
    "flags.DEFINE_integer('dim_1', 128, 'Size of output dim (final is 2x this, if using concat)')\n",
    "flags.DEFINE_integer('dim_2', 128, 'Size of output dim (final is 2x this, if using concat)')\n",
    "flags.DEFINE_boolean('random_context', True, 'Whether to use random context or direct edges')\n",
    "flags.DEFINE_integer('neg_sample_size', 20, 'number of negative samples')\n",
    "flags.DEFINE_integer('batch_size', 512, 'minibatch size.')\n",
    "flags.DEFINE_integer('n2v_test_epochs', 1, 'Number of new SGD epochs for n2v.')\n",
    "flags.DEFINE_integer('identity_dim', 0, 'Set to positive value to use identity embedding features of that dimension. Default 0.')\n",
    "\n",
    "#logging, saving, validation settings etc.\n",
    "flags.DEFINE_boolean('save_embeddings', True, 'whether to save embeddings for all nodes after training')\n",
    "flags.DEFINE_string('base_log_dir', '.', 'base directory for logging and saving embeddings')\n",
    "flags.DEFINE_integer('validate_iter', 5000, \"how often to run a validation minibatch.\")\n",
    "flags.DEFINE_integer('validate_batch_size', 256, \"how many nodes per validation sample.\")\n",
    "flags.DEFINE_integer('gpu', 1, \"which gpu to use.\")\n",
    "flags.DEFINE_integer('print_every', 50, \"How often to print training info.\")\n",
    "flags.DEFINE_integer('max_total_steps', 10**10, \"Maximum total number of iterations\")\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=str(FLAGS.gpu)\n",
    "\n",
    "GPU_MEM_FRACTION = 0.8\n",
    "\n",
    "def log_dir():\n",
    "    log_dir = FLAGS.base_log_dir + \"/unsup-\" + FLAGS.train_prefix.split(\"/\")[-2]\n",
    "    log_dir += \"/{model:s}_{model_size:s}_{lr:0.6f}/\".format(\n",
    "            model=FLAGS.model,\n",
    "            model_size=FLAGS.model_size,\n",
    "            lr=FLAGS.learning_rate)\n",
    "    if not os.path.exists(log_dir):\n",
    "        os.makedirs(log_dir)\n",
    "    return log_dir\n",
    "\n",
    "# Define model evaluation function\n",
    "def evaluate(sess, model, minibatch_iter, size=None):\n",
    "    t_test = time.time()\n",
    "    feed_dict_val = minibatch_iter.val_feed_dict(size)\n",
    "    outs_val = sess.run([model.loss, model.ranks, model.mrr], \n",
    "                        feed_dict=feed_dict_val)\n",
    "    return outs_val[0], outs_val[1], outs_val[2], (time.time() - t_test)\n",
    "\n",
    "def incremental_evaluate(sess, model, minibatch_iter, size):\n",
    "    t_test = time.time()\n",
    "    finished = False\n",
    "    val_losses = []\n",
    "    val_mrrs = []\n",
    "    iter_num = 0\n",
    "    while not finished:\n",
    "        feed_dict_val, finished, _ = minibatch_iter.incremental_val_feed_dict(size, iter_num)\n",
    "        iter_num += 1\n",
    "        outs_val = sess.run([model.loss, model.ranks, model.mrr], \n",
    "                            feed_dict=feed_dict_val)\n",
    "        val_losses.append(outs_val[0])\n",
    "        val_mrrs.append(outs_val[2])\n",
    "    return np.mean(val_losses), np.mean(val_mrrs), (time.time() - t_test)\n",
    "\n",
    "def save_val_embeddings(sess, model, minibatch_iter, size, out_dir, mod=\"\"):\n",
    "    val_embeddings = []\n",
    "    finished = False\n",
    "    seen = set([])\n",
    "    nodes = []\n",
    "    iter_num = 0\n",
    "    name = \"val\"\n",
    "    while not finished:\n",
    "        feed_dict_val, finished, edges = minibatch_iter.incremental_embed_feed_dict(size, iter_num)\n",
    "        iter_num += 1\n",
    "        outs_val = sess.run([model.loss, model.mrr, model.outputs1], \n",
    "                            feed_dict=feed_dict_val)\n",
    "        \n",
    "        #ONLY SAVE FOR embeds1 because of planetoid\n",
    "        for i, edge in enumerate(edges):\n",
    "            if not edge[0] in seen:\n",
    "                val_embeddings.append(outs_val[-1][i,:])\n",
    "                nodes.append(edge[0])\n",
    "                seen.add(edge[0])\n",
    "    if not os.path.exists(out_dir):\n",
    "        os.makedirs(out_dir)\n",
    "    val_embeddings = np.vstack(val_embeddings)\n",
    "    np.save(out_dir + name + mod + \".npy\",  val_embeddings)\n",
    "    with open(out_dir + name + mod + \".txt\", \"w\") as fp:\n",
    "        fp.write(\"\\n\".join(map(str,nodes)))\n",
    "\n",
    "def construct_placeholders():\n",
    "    # Define placeholders\n",
    "    placeholders = {\n",
    "        'batch1' : tf.placeholder(tf.int32, shape=(None), name='batch1'),\n",
    "        'batch2' : tf.placeholder(tf.int32, shape=(None), name='batch2'),\n",
    "        # negative samples for all nodes in the batch\n",
    "        # During training, positive node-context pairs are created using observed interactions in the graph. \n",
    "        # These observed pairs provide the algorithm with examples of nodes and their corresponding contexts.\n",
    "        # Negative sampling, as mentioned earlier, involves randomly selecting unobserved node-context pairs\n",
    "        # to provide contrastive examples for training.\n",
    "        'neg_samples': tf.placeholder(tf.int32, shape=(None,),\n",
    "            name='neg_sample_size'),\n",
    "        'dropout': tf.placeholder_with_default(0., shape=(), name='dropout'),\n",
    "        'batch_size' : tf.placeholder(tf.int32, name='batch_size'),\n",
    "    }\n",
    "    return placeholders\n",
    "\n",
    "def train(train_data, test_data=None):\n",
    "    G = train_data[0]\n",
    "    features = train_data[1]\n",
    "    id_map = train_data[2]\n",
    "\n",
    "    if not features is None:\n",
    "        # pad with dummy zero vector\n",
    "        features = np.vstack([features, np.zeros((features.shape[1],))])\n",
    "\n",
    "    context_pairs = train_data[3] if FLAGS.random_context else None\n",
    "    placeholders = construct_placeholders()\n",
    "    minibatch = EdgeMinibatchIterator(G, \n",
    "            id_map,\n",
    "            placeholders, batch_size=FLAGS.batch_size,\n",
    "            max_degree=FLAGS.max_degree, \n",
    "            num_neg_samples=FLAGS.neg_sample_size,\n",
    "            context_pairs = context_pairs)\n",
    "    adj_info_ph = tf.placeholder(tf.int32, shape=minibatch.adj.shape)\n",
    "    adj_info = tf.Variable(adj_info_ph, trainable=False, name=\"adj_info\")\n",
    "\n",
    "    if FLAGS.model == 'graphsage_mean':\n",
    "        # Create model\n",
    "        sampler = UniformNeighborSampler(adj_info)\n",
    "        layer_infos = [SAGEInfo(\"node\", sampler, FLAGS.samples_1, FLAGS.dim_1),\n",
    "                            SAGEInfo(\"node\", sampler, FLAGS.samples_2, FLAGS.dim_2)]\n",
    "\n",
    "        model = SampleAndAggregate(placeholders, \n",
    "                                     features,\n",
    "                                     adj_info,\n",
    "                                     minibatch.deg,\n",
    "                                     layer_infos=layer_infos, \n",
    "                                     model_size=FLAGS.model_size,\n",
    "                                     identity_dim = FLAGS.identity_dim,\n",
    "                                     logging=True)\n",
    "    elif FLAGS.model == 'gcn':\n",
    "        # Create model\n",
    "        sampler = UniformNeighborSampler(adj_info)\n",
    "        layer_infos = [SAGEInfo(\"node\", sampler, FLAGS.samples_1, 2*FLAGS.dim_1),\n",
    "                            SAGEInfo(\"node\", sampler, FLAGS.samples_2, 2*FLAGS.dim_2)]\n",
    "\n",
    "        model = SampleAndAggregate(placeholders, \n",
    "                                     features,\n",
    "                                     adj_info,\n",
    "                                     minibatch.deg,\n",
    "                                     layer_infos=layer_infos, \n",
    "                                     aggregator_type=\"gcn\",\n",
    "                                     model_size=FLAGS.model_size,\n",
    "                                     identity_dim = FLAGS.identity_dim,\n",
    "                                     concat=False,\n",
    "                                     logging=True)\n",
    "\n",
    "    elif FLAGS.model == 'graphsage_seq':\n",
    "        sampler = UniformNeighborSampler(adj_info)\n",
    "        layer_infos = [SAGEInfo(\"node\", sampler, FLAGS.samples_1, FLAGS.dim_1),\n",
    "                            SAGEInfo(\"node\", sampler, FLAGS.samples_2, FLAGS.dim_2)]\n",
    "\n",
    "        model = SampleAndAggregate(placeholders, \n",
    "                                     features,\n",
    "                                     adj_info,\n",
    "                                     minibatch.deg,\n",
    "                                     layer_infos=layer_infos, \n",
    "                                     identity_dim = FLAGS.identity_dim,\n",
    "                                     aggregator_type=\"seq\",\n",
    "                                     model_size=FLAGS.model_size,\n",
    "                                     logging=True)\n",
    "\n",
    "    elif FLAGS.model == 'graphsage_maxpool':\n",
    "        sampler = UniformNeighborSampler(adj_info)\n",
    "        layer_infos = [SAGEInfo(\"node\", sampler, FLAGS.samples_1, FLAGS.dim_1),\n",
    "                            SAGEInfo(\"node\", sampler, FLAGS.samples_2, FLAGS.dim_2)]\n",
    "\n",
    "        model = SampleAndAggregate(placeholders, \n",
    "                                    features,\n",
    "                                    adj_info,\n",
    "                                    minibatch.deg,\n",
    "                                     layer_infos=layer_infos, \n",
    "                                     aggregator_type=\"maxpool\",\n",
    "                                     model_size=FLAGS.model_size,\n",
    "                                     identity_dim = FLAGS.identity_dim,\n",
    "                                     logging=True)\n",
    "    elif FLAGS.model == 'graphsage_meanpool':\n",
    "        sampler = UniformNeighborSampler(adj_info)\n",
    "        layer_infos = [SAGEInfo(\"node\", sampler, FLAGS.samples_1, FLAGS.dim_1),\n",
    "                            SAGEInfo(\"node\", sampler, FLAGS.samples_2, FLAGS.dim_2)]\n",
    "\n",
    "        model = SampleAndAggregate(placeholders, \n",
    "                                    features,\n",
    "                                    adj_info,\n",
    "                                    minibatch.deg,\n",
    "                                     layer_infos=layer_infos, \n",
    "                                     aggregator_type=\"meanpool\",\n",
    "                                     model_size=FLAGS.model_size,\n",
    "                                     identity_dim = FLAGS.identity_dim,\n",
    "                                     logging=True)\n",
    "\n",
    "    elif FLAGS.model == 'n2v':\n",
    "        model = Node2VecModel(placeholders, features.shape[0],\n",
    "                                       minibatch.deg,\n",
    "                                       #2x because graphsage uses concat\n",
    "                                       nodevec_dim=2*FLAGS.dim_1,\n",
    "                                       lr=FLAGS.learning_rate)\n",
    "    else:\n",
    "        raise Exception('Error: model name unrecognized.')\n",
    "\n",
    "    config = tf.ConfigProto(log_device_placement=FLAGS.log_device_placement)\n",
    "    config.gpu_options.allow_growth = True\n",
    "    #config.gpu_options.per_process_gpu_memory_fraction = GPU_MEM_FRACTION\n",
    "    config.allow_soft_placement = True\n",
    "    \n",
    "    # Initialize session\n",
    "    sess = tf.Session(config=config)\n",
    "    merged = tf.summary.merge_all()\n",
    "    summary_writer = tf.summary.FileWriter(log_dir(), sess.graph)\n",
    "     \n",
    "    # Init variables\n",
    "    sess.run(tf.global_variables_initializer(), feed_dict={adj_info_ph: minibatch.adj})\n",
    "    \n",
    "    # Train model\n",
    "    \n",
    "    train_shadow_mrr = None\n",
    "    shadow_mrr = None\n",
    "\n",
    "    total_steps = 0\n",
    "    avg_time = 0.0\n",
    "    epoch_val_costs = []\n",
    "\n",
    "    train_adj_info = tf.assign(adj_info, minibatch.adj)\n",
    "    val_adj_info = tf.assign(adj_info, minibatch.test_adj)\n",
    "    for epoch in range(FLAGS.epochs): \n",
    "        minibatch.shuffle() \n",
    "\n",
    "        iter = 0\n",
    "        print('Epoch: %04d' % (epoch + 1))\n",
    "        epoch_val_costs.append(0)\n",
    "        while not minibatch.end():\n",
    "            # Construct feed dictionary\n",
    "            feed_dict = minibatch.next_minibatch_feed_dict()\n",
    "            feed_dict.update({placeholders['dropout']: FLAGS.dropout})\n",
    "\n",
    "            t = time.time()\n",
    "            # Training step\n",
    "            outs = sess.run([merged, model.opt_op, model.loss, model.ranks, model.aff_all, \n",
    "                    model.mrr, model.outputs1], feed_dict=feed_dict)\n",
    "            train_cost = outs[2]\n",
    "            train_mrr = outs[5]\n",
    "            if train_shadow_mrr is None:\n",
    "                train_shadow_mrr = train_mrr\n",
    "            else:\n",
    "                train_shadow_mrr -= (1-0.99) * (train_shadow_mrr - train_mrr)\n",
    "\n",
    "            if iter % FLAGS.validate_iter == 0:\n",
    "                # Validation\n",
    "                sess.run(val_adj_info.op)\n",
    "                val_cost, ranks, val_mrr, duration  = evaluate(sess, model, minibatch, size=FLAGS.validate_batch_size)\n",
    "                sess.run(train_adj_info.op)\n",
    "                epoch_val_costs[-1] += val_cost\n",
    "            if shadow_mrr is None:\n",
    "                shadow_mrr = val_mrr\n",
    "            else:\n",
    "                shadow_mrr -= (1-0.99) * (shadow_mrr - val_mrr)\n",
    "\n",
    "            if total_steps % FLAGS.print_every == 0:\n",
    "                summary_writer.add_summary(outs[0], total_steps)\n",
    "    \n",
    "            # Print results\n",
    "            avg_time = (avg_time * total_steps + time.time() - t) / (total_steps + 1)\n",
    "\n",
    "            if total_steps % FLAGS.print_every == 0:\n",
    "                print(\"Iter:\", '%04d' % iter, \n",
    "                      \"train_loss=\", \"{:.5f}\".format(train_cost),\n",
    "                      \"train_mrr=\", \"{:.5f}\".format(train_mrr), \n",
    "                      \"train_mrr_ema=\", \"{:.5f}\".format(train_shadow_mrr), # exponential moving average\n",
    "                      \"val_loss=\", \"{:.5f}\".format(val_cost),\n",
    "                      \"val_mrr=\", \"{:.5f}\".format(val_mrr), \n",
    "                      \"val_mrr_ema=\", \"{:.5f}\".format(shadow_mrr), # exponential moving average\n",
    "                      \"time=\", \"{:.5f}\".format(avg_time))\n",
    "\n",
    "            iter += 1\n",
    "            total_steps += 1\n",
    "\n",
    "            if total_steps > FLAGS.max_total_steps:\n",
    "                break\n",
    "\n",
    "        if total_steps > FLAGS.max_total_steps:\n",
    "                break\n",
    "    \n",
    "    print(\"Optimization Finished!\")\n",
    "    if FLAGS.save_embeddings:\n",
    "        sess.run(val_adj_info.op)\n",
    "\n",
    "        save_val_embeddings(sess, model, minibatch, FLAGS.validate_batch_size, log_dir())\n",
    "\n",
    "        if FLAGS.model == \"n2v\":\n",
    "            # stopping the gradient for the already trained nodes\n",
    "            train_ids = tf.constant([[id_map[n]] for n in G.nodes_iter() if not G.node[n]['val'] and not G.node[n]['test']],\n",
    "                    dtype=tf.int32)\n",
    "            test_ids = tf.constant([[id_map[n]] for n in G.nodes_iter() if G.node[n]['val'] or G.node[n]['test']], \n",
    "                    dtype=tf.int32)\n",
    "            update_nodes = tf.nn.embedding_lookup(model.context_embeds, tf.squeeze(test_ids))\n",
    "            no_update_nodes = tf.nn.embedding_lookup(model.context_embeds,tf.squeeze(train_ids))\n",
    "            update_nodes = tf.scatter_nd(test_ids, update_nodes, tf.shape(model.context_embeds))\n",
    "            no_update_nodes = tf.stop_gradient(tf.scatter_nd(train_ids, no_update_nodes, tf.shape(model.context_embeds)))\n",
    "            model.context_embeds = update_nodes + no_update_nodes\n",
    "            sess.run(model.context_embeds)\n",
    "\n",
    "            # run random walks\n",
    "            from graphsage.utils import run_random_walks\n",
    "            nodes = [n for n in G.nodes_iter() if G.node[n][\"val\"] or G.node[n][\"test\"]]\n",
    "            start_time = time.time()\n",
    "            pairs = run_random_walks(G, nodes, num_walks=50)\n",
    "            walk_time = time.time() - start_time\n",
    "\n",
    "            test_minibatch = EdgeMinibatchIterator(G, \n",
    "                id_map,\n",
    "                placeholders, batch_size=FLAGS.batch_size,\n",
    "                max_degree=FLAGS.max_degree, \n",
    "                num_neg_samples=FLAGS.neg_sample_size,\n",
    "                context_pairs = pairs,\n",
    "                n2v_retrain=True,\n",
    "                fixed_n2v=True)\n",
    "            \n",
    "            start_time = time.time()\n",
    "            print(\"Doing test training for n2v.\")\n",
    "            test_steps = 0\n",
    "            for epoch in range(FLAGS.n2v_test_epochs):\n",
    "                test_minibatch.shuffle()\n",
    "                while not test_minibatch.end():\n",
    "                    feed_dict = test_minibatch.next_minibatch_feed_dict()\n",
    "                    feed_dict.update({placeholders['dropout']: FLAGS.dropout})\n",
    "                    outs = sess.run([model.opt_op, model.loss, model.ranks, model.aff_all, \n",
    "                        model.mrr, model.outputs1], feed_dict=feed_dict)\n",
    "                    if test_steps % FLAGS.print_every == 0:\n",
    "                        print(\"Iter:\", '%04d' % test_steps, \n",
    "                              \"train_loss=\", \"{:.5f}\".format(outs[1]),\n",
    "                              \"train_mrr=\", \"{:.5f}\".format(outs[-2]))\n",
    "                    test_steps += 1\n",
    "            train_time = time.time() - start_time\n",
    "            save_val_embeddings(sess, model, minibatch, FLAGS.validate_batch_size, log_dir(), mod=\"-test\")\n",
    "            print(\"Total time: \", train_time+walk_time)\n",
    "            print(\"Walk time: \", walk_time)\n",
    "            print(\"Train time: \", train_time)\n",
    "\n",
    "    \n",
    "\n",
    "def main(argv=None):\n",
    "    print(\"Loading training data..\")\n",
    "    train_data = load_data(FLAGS.train_prefix, load_walks=True)\n",
    "    print(\"Done loading training data..\")\n",
    "    train(train_data)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    tf.app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c5699b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get model for supervised Train\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn import metrics\n",
    "\n",
    "from graphsage.supervised_models import SupervisedGraphsage\n",
    "from graphsage.models import SAGEInfo\n",
    "from graphsage.minibatch import NodeMinibatchIterator\n",
    "from graphsage.neigh_samplers import UniformNeighborSampler\n",
    "from graphsage.utils import load_data\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "\n",
    "# Set random seed\n",
    "seed = 123\n",
    "np.random.seed(seed)\n",
    "tf.set_random_seed(seed)\n",
    "\n",
    "# Settings\n",
    "flags = tf.app.flags\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "tf.app.flags.DEFINE_boolean('log_device_placement', False,\n",
    "                            \"\"\"Whether to log device placement.\"\"\")\n",
    "#core params..\n",
    "flags.DEFINE_string('model', 'graphsage_mean', 'model names. See README for possible values.')  \n",
    "flags.DEFINE_float('learning_rate', 0.01, 'initial learning rate.')\n",
    "flags.DEFINE_string(\"model_size\", \"small\", \"Can be big or small; model specific def'ns\")\n",
    "flags.DEFINE_string('train_prefix', '', 'prefix identifying training data. must be specified.')\n",
    "\n",
    "# left to default values in main experiments \n",
    "flags.DEFINE_integer('epochs', 10, 'number of epochs to train.')\n",
    "flags.DEFINE_float('dropout', 0.0, 'dropout rate (1 - keep probability).')\n",
    "flags.DEFINE_float('weight_decay', 0.0, 'weight for l2 loss on embedding matrix.')\n",
    "flags.DEFINE_integer('max_degree', 128, 'maximum node degree.')\n",
    "flags.DEFINE_integer('samples_1', 25, 'number of samples in layer 1')\n",
    "flags.DEFINE_integer('samples_2', 10, 'number of samples in layer 2')\n",
    "flags.DEFINE_integer('samples_3', 0, 'number of users samples in layer 3. (Only for mean model)')\n",
    "flags.DEFINE_integer('dim_1', 128, 'Size of output dim (final is 2x this, if using concat)')\n",
    "flags.DEFINE_integer('dim_2', 128, 'Size of output dim (final is 2x this, if using concat)')\n",
    "flags.DEFINE_boolean('random_context', True, 'Whether to use random context or direct edges')\n",
    "flags.DEFINE_integer('batch_size', 512, 'minibatch size.')\n",
    "flags.DEFINE_boolean('sigmoid', False, 'whether to use sigmoid loss')\n",
    "flags.DEFINE_integer('identity_dim', 0, 'Set to positive value to use identity embedding features of that dimension. Default 0.')\n",
    "\n",
    "#logging, saving, validation settings etc.\n",
    "flags.DEFINE_string('base_log_dir', '.', 'base directory for logging and saving embeddings')\n",
    "flags.DEFINE_integer('validate_iter', 5000, \"how often to run a validation minibatch.\")\n",
    "flags.DEFINE_integer('validate_batch_size', 256, \"how many nodes per validation sample.\")\n",
    "flags.DEFINE_integer('gpu', 1, \"which gpu to use.\")\n",
    "flags.DEFINE_integer('print_every', 5, \"How often to print training info.\")\n",
    "flags.DEFINE_integer('max_total_steps', 10**10, \"Maximum total number of iterations\")\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=str(FLAGS.gpu)\n",
    "\n",
    "GPU_MEM_FRACTION = 0.8\n",
    "\n",
    "def calc_f1(y_true, y_pred):\n",
    "    if not FLAGS.sigmoid:\n",
    "        y_true = np.argmax(y_true, axis=1)\n",
    "        y_pred = np.argmax(y_pred, axis=1)\n",
    "    else:\n",
    "        y_pred[y_pred > 0.5] = 1\n",
    "        y_pred[y_pred <= 0.5] = 0\n",
    "    return metrics.f1_score(y_true, y_pred, average=\"micro\"), metrics.f1_score(y_true, y_pred, average=\"macro\")\n",
    "\n",
    "# Define model evaluation function\n",
    "def evaluate(sess, model, minibatch_iter, size=None):\n",
    "    t_test = time.time()\n",
    "    feed_dict_val, labels = minibatch_iter.node_val_feed_dict(size)\n",
    "    node_outs_val = sess.run([model.preds, model.loss], \n",
    "                        feed_dict=feed_dict_val)\n",
    "    mic, mac = calc_f1(labels, node_outs_val[0])\n",
    "    return node_outs_val[1], mic, mac, (time.time() - t_test)\n",
    "\n",
    "def log_dir():\n",
    "    log_dir = FLAGS.base_log_dir + \"/sup-\" + FLAGS.train_prefix.split(\"/\")[-2]\n",
    "    log_dir += \"/{model:s}_{model_size:s}_{lr:0.4f}/\".format(\n",
    "            model=FLAGS.model,\n",
    "            model_size=FLAGS.model_size,\n",
    "            lr=FLAGS.learning_rate)\n",
    "    if not os.path.exists(log_dir):\n",
    "        os.makedirs(log_dir)\n",
    "    return log_dir\n",
    "\n",
    "def incremental_evaluate(sess, model, minibatch_iter, size, test=False):\n",
    "    t_test = time.time()\n",
    "    finished = False\n",
    "    val_losses = []\n",
    "    val_preds = []\n",
    "    labels = []\n",
    "    iter_num = 0\n",
    "    finished = False\n",
    "    while not finished:\n",
    "        feed_dict_val, batch_labels, finished, _  = minibatch_iter.incremental_node_val_feed_dict(size, iter_num, test=test)\n",
    "        node_outs_val = sess.run([model.preds, model.loss], \n",
    "                         feed_dict=feed_dict_val)\n",
    "        val_preds.append(node_outs_val[0])\n",
    "        labels.append(batch_labels)\n",
    "        val_losses.append(node_outs_val[1])\n",
    "        iter_num += 1\n",
    "    val_preds = np.vstack(val_preds)\n",
    "    labels = np.vstack(labels)\n",
    "    f1_scores = calc_f1(labels, val_preds)\n",
    "    return np.mean(val_losses), f1_scores[0], f1_scores[1], (time.time() - t_test)\n",
    "\n",
    "def construct_placeholders(num_classes):\n",
    "    # Define placeholders\n",
    "    placeholders = {\n",
    "        'labels' : tf.placeholder(tf.float32, shape=(None, num_classes), name='labels'),\n",
    "        'batch' : tf.placeholder(tf.int32, shape=(None), name='batch1'),\n",
    "        'dropout': tf.placeholder_with_default(0., shape=(), name='dropout'),\n",
    "        'batch_size' : tf.placeholder(tf.int32, name='batch_size'),\n",
    "    }\n",
    "    return placeholders\n",
    "\n",
    "def train(train_data, test_data=None):\n",
    "\n",
    "    G = train_data[0]\n",
    "    features = train_data[1]\n",
    "    id_map = train_data[2]\n",
    "    class_map  = train_data[4]\n",
    "    if isinstance(list(class_map.values())[0], list):\n",
    "        num_classes = len(list(class_map.values())[0])\n",
    "    else:\n",
    "        num_classes = len(set(class_map.values()))\n",
    "\n",
    "    if not features is None:\n",
    "        # pad with dummy zero vector\n",
    "        features = np.vstack([features, np.zeros((features.shape[1],))])\n",
    "\n",
    "    context_pairs = train_data[3] if FLAGS.random_context else None\n",
    "    placeholders = construct_placeholders(num_classes)\n",
    "    minibatch = NodeMinibatchIterator(G, \n",
    "            id_map,\n",
    "            placeholders, \n",
    "            class_map,\n",
    "            num_classes,\n",
    "            batch_size=FLAGS.batch_size,\n",
    "            max_degree=FLAGS.max_degree, \n",
    "            context_pairs = context_pairs)\n",
    "    adj_info_ph = tf.placeholder(tf.int32, shape=minibatch.adj.shape)\n",
    "    adj_info = tf.Variable(adj_info_ph, trainable=False, name=\"adj_info\")\n",
    "\n",
    "    if FLAGS.model == 'graphsage_mean':\n",
    "        # Create model\n",
    "        sampler = UniformNeighborSampler(adj_info)\n",
    "        if FLAGS.samples_3 != 0:\n",
    "            layer_infos = [SAGEInfo(\"node\", sampler, FLAGS.samples_1, FLAGS.dim_1),\n",
    "                                SAGEInfo(\"node\", sampler, FLAGS.samples_2, FLAGS.dim_2),\n",
    "                                SAGEInfo(\"node\", sampler, FLAGS.samples_3, FLAGS.dim_2)]\n",
    "        elif FLAGS.samples_2 != 0:\n",
    "            layer_infos = [SAGEInfo(\"node\", sampler, FLAGS.samples_1, FLAGS.dim_1),\n",
    "                                SAGEInfo(\"node\", sampler, FLAGS.samples_2, FLAGS.dim_2)]\n",
    "        else:\n",
    "            layer_infos = [SAGEInfo(\"node\", sampler, FLAGS.samples_1, FLAGS.dim_1)]\n",
    "\n",
    "        model = SupervisedGraphsage(num_classes, placeholders, \n",
    "                                     features,\n",
    "                                     adj_info,\n",
    "                                     minibatch.deg,\n",
    "                                     layer_infos, \n",
    "                                     model_size=FLAGS.model_size,\n",
    "                                     sigmoid_loss = FLAGS.sigmoid,\n",
    "                                     identity_dim = FLAGS.identity_dim,\n",
    "                                     logging=True)\n",
    "    elif FLAGS.model == 'gcn':\n",
    "        # Create model\n",
    "        sampler = UniformNeighborSampler(adj_info)\n",
    "        layer_infos = [SAGEInfo(\"node\", sampler, FLAGS.samples_1, 2*FLAGS.dim_1),\n",
    "                            SAGEInfo(\"node\", sampler, FLAGS.samples_2, 2*FLAGS.dim_2)]\n",
    "\n",
    "        model = SupervisedGraphsage(num_classes, placeholders, \n",
    "                                     features,\n",
    "                                     adj_info,\n",
    "                                     minibatch.deg,\n",
    "                                     layer_infos=layer_infos, \n",
    "                                     aggregator_type=\"gcn\",\n",
    "                                     model_size=FLAGS.model_size,\n",
    "                                     concat=False,\n",
    "                                     sigmoid_loss = FLAGS.sigmoid,\n",
    "                                     identity_dim = FLAGS.identity_dim,\n",
    "                                     logging=True)\n",
    "\n",
    "    elif FLAGS.model == 'graphsage_seq':\n",
    "        sampler = UniformNeighborSampler(adj_info)\n",
    "        layer_infos = [SAGEInfo(\"node\", sampler, FLAGS.samples_1, FLAGS.dim_1),\n",
    "                            SAGEInfo(\"node\", sampler, FLAGS.samples_2, FLAGS.dim_2)]\n",
    "\n",
    "        model = SupervisedGraphsage(num_classes, placeholders, \n",
    "                                     features,\n",
    "                                     adj_info,\n",
    "                                     minibatch.deg,\n",
    "                                     layer_infos=layer_infos, \n",
    "                                     aggregator_type=\"seq\",\n",
    "                                     model_size=FLAGS.model_size,\n",
    "                                     sigmoid_loss = FLAGS.sigmoid,\n",
    "                                     identity_dim = FLAGS.identity_dim,\n",
    "                                     logging=True)\n",
    "\n",
    "    elif FLAGS.model == 'graphsage_maxpool':\n",
    "        sampler = UniformNeighborSampler(adj_info)\n",
    "        layer_infos = [SAGEInfo(\"node\", sampler, FLAGS.samples_1, FLAGS.dim_1),\n",
    "                            SAGEInfo(\"node\", sampler, FLAGS.samples_2, FLAGS.dim_2)]\n",
    "\n",
    "        model = SupervisedGraphsage(num_classes, placeholders, \n",
    "                                    features,\n",
    "                                    adj_info,\n",
    "                                    minibatch.deg,\n",
    "                                     layer_infos=layer_infos, \n",
    "                                     aggregator_type=\"maxpool\",\n",
    "                                     model_size=FLAGS.model_size,\n",
    "                                     sigmoid_loss = FLAGS.sigmoid,\n",
    "                                     identity_dim = FLAGS.identity_dim,\n",
    "                                     logging=True)\n",
    "\n",
    "    elif FLAGS.model == 'graphsage_meanpool':\n",
    "        sampler = UniformNeighborSampler(adj_info)\n",
    "        layer_infos = [SAGEInfo(\"node\", sampler, FLAGS.samples_1, FLAGS.dim_1),\n",
    "                            SAGEInfo(\"node\", sampler, FLAGS.samples_2, FLAGS.dim_2)]\n",
    "\n",
    "        model = SupervisedGraphsage(num_classes, placeholders, \n",
    "                                    features,\n",
    "                                    adj_info,\n",
    "                                    minibatch.deg,\n",
    "                                     layer_infos=layer_infos, \n",
    "                                     aggregator_type=\"meanpool\",\n",
    "                                     model_size=FLAGS.model_size,\n",
    "                                     sigmoid_loss = FLAGS.sigmoid,\n",
    "                                     identity_dim = FLAGS.identity_dim,\n",
    "                                     logging=True)\n",
    "\n",
    "    else:\n",
    "        raise Exception('Error: model name unrecognized.')\n",
    "\n",
    "    config = tf.ConfigProto(log_device_placement=FLAGS.log_device_placement)\n",
    "    config.gpu_options.allow_growth = True\n",
    "    #config.gpu_options.per_process_gpu_memory_fraction = GPU_MEM_FRACTION\n",
    "    config.allow_soft_placement = True\n",
    "    \n",
    "    # Initialize session\n",
    "    sess = tf.Session(config=config)\n",
    "    merged = tf.summary.merge_all()\n",
    "    summary_writer = tf.summary.FileWriter(log_dir(), sess.graph)\n",
    "     \n",
    "    # Init variables\n",
    "    sess.run(tf.global_variables_initializer(), feed_dict={adj_info_ph: minibatch.adj})\n",
    "    \n",
    "    # Train model\n",
    "    \n",
    "    total_steps = 0\n",
    "    avg_time = 0.0\n",
    "    epoch_val_costs = []\n",
    "\n",
    "    train_adj_info = tf.assign(adj_info, minibatch.adj)\n",
    "    val_adj_info = tf.assign(adj_info, minibatch.test_adj)\n",
    "    for epoch in range(FLAGS.epochs): \n",
    "        minibatch.shuffle() \n",
    "\n",
    "        iter = 0\n",
    "        print('Epoch: %04d' % (epoch + 1))\n",
    "        epoch_val_costs.append(0)\n",
    "        while not minibatch.end():\n",
    "            # Construct feed dictionary\n",
    "            feed_dict, labels = minibatch.next_minibatch_feed_dict()\n",
    "            feed_dict.update({placeholders['dropout']: FLAGS.dropout})\n",
    "\n",
    "            t = time.time()\n",
    "            # Training step\n",
    "            outs = sess.run([merged, model.opt_op, model.loss, model.preds], feed_dict=feed_dict)\n",
    "            train_cost = outs[2]\n",
    "\n",
    "            if iter % FLAGS.validate_iter == 0:\n",
    "                # Validation\n",
    "                sess.run(val_adj_info.op)\n",
    "                if FLAGS.validate_batch_size == -1:\n",
    "                    val_cost, val_f1_mic, val_f1_mac, duration = incremental_evaluate(sess, model, minibatch, FLAGS.batch_size)\n",
    "                else:\n",
    "                    val_cost, val_f1_mic, val_f1_mac, duration = evaluate(sess, model, minibatch, FLAGS.validate_batch_size)\n",
    "                sess.run(train_adj_info.op)\n",
    "                epoch_val_costs[-1] += val_cost\n",
    "\n",
    "            if total_steps % FLAGS.print_every == 0:\n",
    "                summary_writer.add_summary(outs[0], total_steps)\n",
    "    \n",
    "            # Print results\n",
    "            avg_time = (avg_time * total_steps + time.time() - t) / (total_steps + 1)\n",
    "\n",
    "            if total_steps % FLAGS.print_every == 0:\n",
    "                train_f1_mic, train_f1_mac = calc_f1(labels, outs[-1])\n",
    "                print(\"Iter:\", '%04d' % iter, \n",
    "                      \"train_loss=\", \"{:.5f}\".format(train_cost),\n",
    "                      \"train_f1_mic=\", \"{:.5f}\".format(train_f1_mic), \n",
    "                      \"train_f1_mac=\", \"{:.5f}\".format(train_f1_mac), \n",
    "                      \"val_loss=\", \"{:.5f}\".format(val_cost),\n",
    "                      \"val_f1_mic=\", \"{:.5f}\".format(val_f1_mic), \n",
    "                      \"val_f1_mac=\", \"{:.5f}\".format(val_f1_mac), \n",
    "                      \"time=\", \"{:.5f}\".format(avg_time))\n",
    " \n",
    "            iter += 1\n",
    "            total_steps += 1\n",
    "\n",
    "            if total_steps > FLAGS.max_total_steps:\n",
    "                break\n",
    "\n",
    "        if total_steps > FLAGS.max_total_steps:\n",
    "                break\n",
    "    \n",
    "    print(\"Optimization Finished!\")\n",
    "    sess.run(val_adj_info.op)\n",
    "    val_cost, val_f1_mic, val_f1_mac, duration = incremental_evaluate(sess, model, minibatch, FLAGS.batch_size)\n",
    "    print(\"Full validation stats:\",\n",
    "                  \"loss=\", \"{:.5f}\".format(val_cost),\n",
    "                  \"f1_micro=\", \"{:.5f}\".format(val_f1_mic),\n",
    "                  \"f1_macro=\", \"{:.5f}\".format(val_f1_mac),\n",
    "                  \"time=\", \"{:.5f}\".format(duration))\n",
    "    with open(log_dir() + \"val_stats.txt\", \"w\") as fp:\n",
    "        fp.write(\"loss={:.5f} f1_micro={:.5f} f1_macro={:.5f} time={:.5f}\".\n",
    "                format(val_cost, val_f1_mic, val_f1_mac, duration))\n",
    "\n",
    "    print(\"Writing test set stats to file (don't peak!)\")\n",
    "    val_cost, val_f1_mic, val_f1_mac, duration = incremental_evaluate(sess, model, minibatch, FLAGS.batch_size, test=True)\n",
    "    with open(log_dir() + \"test_stats.txt\", \"w\") as fp:\n",
    "        fp.write(\"loss={:.5f} f1_micro={:.5f} f1_macro={:.5f}\".\n",
    "                format(val_cost, val_f1_mic, val_f1_mac))\n",
    "\n",
    "def main(argv=None):\n",
    "    print(\"Loading training data..\")\n",
    "    train_data = load_data(FLAGS.train_prefix)\n",
    "    print(\"Done loading training data..\")\n",
    "    train(train_data)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    tf.app.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
