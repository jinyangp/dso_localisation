{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# for kaggle\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"execution":{"iopub.status.busy":"2023-06-13T13:13:12.604926Z","iopub.execute_input":"2023-06-13T13:13:12.606926Z","iopub.status.idle":"2023-06-13T13:13:12.640191Z","shell.execute_reply.started":"2023-06-13T13:13:12.606897Z","shell.execute_reply":"2023-06-13T13:13:12.639374Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# GPU Configuration","metadata":{}},{"cell_type":"code","source":"# Configure amd test GPU\nimport tensorflow as tf\nfrom tensorflow.python.client import device_lib\n\n# Prevent automatic GPU memory pre-allocation\ngpus = tf.config.experimental.list_physical_devices('GPU')\nfor gpu in gpus:\n    print(gpu)\n    tf.config.experimental.set_memory_growth(gpu, True)\n\nprint(tf.__version__)\n# print(device_lib.list_local_devices())","metadata":{"execution":{"iopub.status.busy":"2023-06-13T13:13:33.875932Z","iopub.execute_input":"2023-06-13T13:13:33.876267Z","iopub.status.idle":"2023-06-13T13:13:49.958495Z","shell.execute_reply.started":"2023-06-13T13:13:33.876241Z","shell.execute_reply":"2023-06-13T13:13:49.957323Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install keras-tuner","metadata":{"execution":{"iopub.status.busy":"2023-06-13T13:13:49.960253Z","iopub.execute_input":"2023-06-13T13:13:49.961131Z","iopub.status.idle":"2023-06-13T13:14:02.978797Z","shell.execute_reply.started":"2023-06-13T13:13:49.961094Z","shell.execute_reply":"2023-06-13T13:14:02.977630Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\n\nos.chdir('../input/dataset-snr20-outdoor')\nprint(os.getcwd())","metadata":{"execution":{"iopub.status.busy":"2023-06-13T13:14:06.130649Z","iopub.execute_input":"2023-06-13T13:14:06.131029Z","iopub.status.idle":"2023-06-13T13:14:06.138860Z","shell.execute_reply.started":"2023-06-13T13:14:06.130997Z","shell.execute_reply":"2023-06-13T13:14:06.137881Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Encapsulating tuner into a class\n\n## Parameters to be tuned:\n- Regularisation and Dropout\n    - l2 regularisation on Conv2D layers\n    - Dropout layers after Max Pooling layers and before Fully Connected Layer, just before Softmax output\n- Learning rate of optimiser\n- Batch size\n- Number of epochs (Using EarlyStopping keras callback","metadata":{}},{"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.utils import np_utils\n\nimport h5py\nimport numpy as np\nimport math\nimport os\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\nimport keras_tuner as kt","metadata":{"execution":{"iopub.status.busy":"2023-06-13T13:14:08.478789Z","iopub.execute_input":"2023-06-13T13:14:08.479146Z","iopub.status.idle":"2023-06-13T13:14:09.494274Z","shell.execute_reply.started":"2023-06-13T13:14:08.479118Z","shell.execute_reply":"2023-06-13T13:14:09.493300Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get dictionary of RP index and coordinates\n# Open HDF5 file and access the dataset\nfilename = 'dataset_SNR20_outdoor.mat'\nhdf5_file = h5py.File(filename, 'r')\n\nfeatures_dataset = hdf5_file['features']\nlabels_dataset = hdf5_file['labels']['position']\n\n# Convert HDF5 dataset to NumPy array\nfeatures = np.array(features_dataset)\nlabels = np.array(labels_dataset)\n\n# Prepare features for dataset\n# Retrieve features from the first UE and transpose the individual matrix\nfeatures_transposed = np.zeros((3876,193,16), dtype = np.float64)\nfor i in range(len(features)):\n    features_transposed[i] = features[i][0].T\n\n# Prepare labels for dataset\ncount = 0\nrp_dict = {}\n# For labels, have a shape of (1,) where that number represents the class of that coordinate\n\nfor label in labels:\n    rp_dict[count] = label\n    count += 1\n\n# Close the HDF5 file\nhdf5_file.close()","metadata":{"execution":{"iopub.status.busy":"2023-06-13T13:14:47.703392Z","iopub.execute_input":"2023-06-13T13:14:47.703760Z","iopub.status.idle":"2023-06-13T13:15:01.341532Z","shell.execute_reply.started":"2023-06-13T13:14:47.703731Z","shell.execute_reply":"2023-06-13T13:15:01.340590Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(os.getcwd())\nos.chdir('../augmented-outdoor-dataset')\nprint(os.getcwd())","metadata":{"execution":{"iopub.status.busy":"2023-06-13T13:15:09.821563Z","iopub.execute_input":"2023-06-13T13:15:09.822513Z","iopub.status.idle":"2023-06-13T13:15:09.832220Z","shell.execute_reply.started":"2023-06-13T13:15:09.822480Z","shell.execute_reply":"2023-06-13T13:15:09.831124Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load datasets\nfeatures = np.load('augmented_features_10_ds.npy')\nlabels = np.load('augmented_labels_10_ds.npy')\n\nprint(f'Shape of features np array: {features.shape}')\nprint(f'Shape of labels np array: {labels.shape}')","metadata":{"execution":{"iopub.status.busy":"2023-06-13T13:15:12.504397Z","iopub.execute_input":"2023-06-13T13:15:12.504786Z","iopub.status.idle":"2023-06-13T13:15:31.623615Z","shell.execute_reply.started":"2023-06-13T13:15:12.504757Z","shell.execute_reply":"2023-06-13T13:15:31.622679Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX = features\ny = labels\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2023-06-13T13:15:34.878431Z","iopub.execute_input":"2023-06-13T13:15:34.878842Z","iopub.status.idle":"2023-06-13T13:15:35.484843Z","shell.execute_reply.started":"2023-06-13T13:15:34.878814Z","shell.execute_reply":"2023-06-13T13:15:35.483894Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Method 1\nclass HyperModel(kt.HyperModel):\n    \n    def build(self,hp):\n        # Initialise the weights of neural network layers\n        # VarianceScaling is a particular method used to initialise weights\n        kaiming_normal = keras.initializers.VarianceScaling(scale=2.0, mode='fan_out', distribution='untruncated_normal', seed = 42)\n\n        # Retrieve hyperparameters to be tuned\n        l2 = hp.Float(\"l2\", min_value=0, max_value=0.01, step=0.01)\n        pooling_dropout = hp.Boolean('conv_dropout', default = False)\n        fc_dropout = hp.Boolean('fc_dropout', default = False)\n        lr = hp.Float(\"learning_rate\", min_value=0.001, max_value=0.01, step=0.005)\n        \n        # Make 3x3 convolutional filters\n        def conv3x3(x, out_planes, stride=1,name=None):\n            x = layers.ZeroPadding2D(padding=1, name=f'{name}_pad')(x)\n\n            # Make 2D convolution layer\n            # return layers.Conv2D(filters=out_planes, kernel_size=3, strides=stride, use_bias=False,\n            #                     kernel_initializer=kaiming_normal, name=name)(x)\n            return layers.Conv2D(filters=out_planes, kernel_size=3, strides=stride, use_bias=False,\n                                 kernel_initializer=kaiming_normal, kernel_regularizer=keras.regularizers.L2(l2),\n                                 name=name)(x)\n\n        def basic_block(x, planes, stride=1, downsample=None, name=None):\n            identity = x\n\n            out = conv3x3(x, planes, stride=stride, name=f'{name}.conv1')\n            out = layers.BatchNormalization(momentum=0.9, epsilon=1e-5, name=f'{name}.bn1')(out)\n            out = layers.ReLU(name=f'{name}.relu1')(out)\n\n            out = conv3x3(out, planes, name=f'{name}.conv2')\n            out = layers.BatchNormalization(momentum=0.9, epsilon=1e-5, name=f'{name}.bn2')(out)\n\n            if downsample is not None:\n                # Create an identical layer for each layer in the downsample\n                for layer in downsample:\n                    identity = layer(identity)\n\n            # Performs element-wise addition of multiple inputs. It is used to combine or merge the outputs of two or more layers by adding them together\n            out = layers.Add(name=f'{name}.add')([identity, out])    \n            out = layers.ReLU(name=f'{name}.relu2')(out)\n\n            return out\n\n        def make_layer(x, planes, blocks, stride=1, name=None):\n            downsample = None\n\n            # inplanes refer to the number of channels in filters\n            inplanes = x.shape[3]\n\n            # Check whether we are downsampling our data (i.e. not going through every elememt)\n            # This happens under two circumstances:\n            # 1. when stride != 1\n            # 2. when the layer we want to make has no. of channels less than the no. of channels input has\n            if stride != 1 or inplanes != planes:\n                # downsample consists of a Conv2D layer and a BatchNormalization layer\n                downsample = [\n                    layers.Conv2D(filters=planes, kernel_size=1, strides=stride, use_bias=False, kernel_initializer=kaiming_normal,\n                                  kernel_regularizer=keras.regularizers.L2(l2),\n                                  name=f'{name}.0.downsample.0'),\n#                    layers.Conv2D(filters=planes, kernel_size=1, strides=stride, use_bias=False, kernel_initializer=kaiming_normal,\n#                                  name=f'{name}.0.downsample.0'),\n                    layers.BatchNormalization(momentum=0.9, epsilon=1e-5, name=f'{name}.0.downsample.1'),\n                ]\n\n            # If no downsample, downsample = None\n\n            x = basic_block(x, planes, stride, downsample, name=f'{name}.0')\n            for i in range(1, blocks):\n                x = basic_block(x, planes, name=f'{name}.{i}')\n\n            return x\n\n        def resnet(x, blocks_per_layer, num_classes=1000):\n\n            # ---------------------------------\n            # Initial entry block\n            x = layers.ZeroPadding2D(padding=3, name='conv1_pad')(x)\n            x = layers.Conv2D(filters=64, kernel_size=7, strides=2, use_bias=False,\n                              kernel_initializer=kaiming_normal,\n                              kernel_regularizer=keras.regularizers.L2(l2),\n                              name='conv1')(x)\n            # x = layers.Conv2D(filters=64, kernel_size=7, strides=2, use_bias=False,\n            #                  kernel_initializer=kaiming_normal,\n            #                  name='conv1')(x)\n            x = layers.BatchNormalization(momentum=0.9, epsilon=1e-5, name='bn1')(x)\n            x = layers.ReLU(name='relu1')(x)\n            x = layers.ZeroPadding2D(padding=1, name='maxpool_pad')(x)\n            x = layers.MaxPool2D(pool_size=3, strides=2, name='maxpool')(x)\n\n            # Additional - Addition of a dropout layer\n            if pooling_dropout:\n                x = layers.Dropout(rate = 0.2)(x)   \n            # ---------------------------------\n\n            # ---------------------------------\n            # This block of code creates the ResNet blocks\n            # In ResNet-18, only have 2 layers per block\n            x = make_layer(x, 64, blocks_per_layer[0], name='layer1')\n            x = make_layer(x, 128, blocks_per_layer[1], stride=2, name='layer2')\n            x = make_layer(x, 256, blocks_per_layer[2], stride=2, name='layer3')\n            x = make_layer(x, 512, blocks_per_layer[3], stride=2, name='layer4')\n            # ---------------------------------\n\n            # ---------------------------------\n            x = layers.GlobalAveragePooling2D(name='avgpool')(x)\n\n            # Addition of a dropout layer\n            if pooling_dropout:\n                x = layers.Dropout(rate = 0.2)(x)    \n\n            initializer = keras.initializers.RandomUniform(-1.0 / math.sqrt(512), 1.0 / math.sqrt(512))\n            x = layers.Dense(units=num_classes, kernel_initializer=initializer, bias_initializer=initializer, name='fc')(x)\n\n            # Additional - Addition of a dropout layer\n            if fc_dropout:\n                x = layers.Dropout(rate = 0.5)(x)  \n\n            # Softmax output layer\n            x = layers.Dense(units=num_classes, activation='softmax')(x)  \n            # ---------------------------------\n\n            return x\n\n        def resnet18(x, **kwargs):\n            return resnet(x, [2, 2, 2, 2], **kwargs)\n\n        def resnet34(x, **kwargs):\n            return resnet(x, [3, 4, 6, 3], **kwargs)\n\n        # Create model\n        model_inputs = keras.Input(shape = (193, 16, 1))\n        model_outputs = resnet18(model_inputs, num_classes = 3876)\n        resnet18_model = keras.Model(model_inputs, model_outputs)\n\n        # Compile model - Classification\n        # default learning rate is 1e-3 = 0.001\n        optimizer = tf.keras.optimizers.Adam(learning_rate = lr)\n        \n        resnet18_model.compile(optimizer=optimizer,\n              loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n              metrics=['accuracy'])\n\n        return resnet18_model\n    \n    def fit(self, hp, model, X_train, y_train, validation_data = None, **kwargs):\n        \n        return model.fit(X_train, y_train,\n                        validation_data = validation_data,\n                        batch_size = hp.Choice('batch_size', [16,32,64]),\n                        **kwargs,\n                        )","metadata":{"execution":{"iopub.status.busy":"2023-06-13T13:15:46.230171Z","iopub.execute_input":"2023-06-13T13:15:46.230518Z","iopub.status.idle":"2023-06-13T13:15:46.260466Z","shell.execute_reply.started":"2023-06-13T13:15:46.230490Z","shell.execute_reply":"2023-06-13T13:15:46.259256Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Must change back to working directory for below code to run\nprint(os.getcwd())\nos.chdir('../../working')\nprint(os.getcwd())","metadata":{"execution":{"iopub.status.busy":"2023-06-13T13:15:54.382485Z","iopub.execute_input":"2023-06-13T13:15:54.383171Z","iopub.status.idle":"2023-06-13T13:15:54.389163Z","shell.execute_reply.started":"2023-06-13T13:15:54.383138Z","shell.execute_reply":"2023-06-13T13:15:54.387917Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Method 1\ntuner = kt.RandomSearch(\n    HyperModel(),\n    objective = 'val_loss',\n    max_trials = 30,\n)","metadata":{"execution":{"iopub.status.busy":"2023-06-13T13:15:56.718413Z","iopub.execute_input":"2023-06-13T13:15:56.718779Z","iopub.status.idle":"2023-06-13T13:16:02.447115Z","shell.execute_reply.started":"2023-06-13T13:15:56.718752Z","shell.execute_reply":"2023-06-13T13:16:02.446153Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tuner.search_space_summary()","metadata":{"execution":{"iopub.status.busy":"2023-06-13T13:16:05.093078Z","iopub.execute_input":"2023-06-13T13:16:05.093969Z","iopub.status.idle":"2023-06-13T13:16:05.100463Z","shell.execute_reply.started":"2023-06-13T13:16:05.093932Z","shell.execute_reply":"2023-06-13T13:16:05.099302Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\ntuner.search(X_train, y_train,\n            validation_data = (X_test, y_test),\n            epochs = 100,\n            callbacks = [stop_early])","metadata":{"execution":{"iopub.status.busy":"2023-06-13T13:16:10.911261Z","iopub.execute_input":"2023-06-13T13:16:10.911606Z","iopub.status.idle":"2023-06-13T13:16:19.835650Z","shell.execute_reply.started":"2023-06-13T13:16:10.911578Z","shell.execute_reply":"2023-06-13T13:16:19.832694Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tuner.results_summary()","metadata":{"execution":{"iopub.status.busy":"2023-06-13T13:16:20.532610Z","iopub.execute_input":"2023-06-13T13:16:20.532990Z","iopub.status.idle":"2023-06-13T13:16:20.539401Z","shell.execute_reply.started":"2023-06-13T13:16:20.532960Z","shell.execute_reply":"2023-06-13T13:16:20.538380Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### End of method 1","metadata":{}},{"cell_type":"code","source":"# Method 2\n'''\ndef build_model(hp):\n    # Initialise the weights of neural network layers\n    # VarianceScaling is a particular method used to initialise weights\n    kaiming_normal = keras.initializers.VarianceScaling(scale=2.0, mode='fan_out', distribution='untruncated_normal', seed = 42)\n\n    # Retrieve hyperparameters to be tuned\n    l2 = hp.Float(\"l2\", min_value=0, max_value=0.01, step=0.01)\n    pooling_dropout = hp.Boolean('conv_dropout', default = False)\n    fc_dropout = hp.Boolean('fc_dropout', default = False)\n    lr = hp.Float(\"learning_rate\", min_value=0.001, max_value=0.01, step=0.005)\n\n    # Make 3x3 convolutional filters\n    def conv3x3(x, out_planes, stride=1,name=None):\n        x = layers.ZeroPadding2D(padding=1, name=f'{name}_pad')(x)\n\n        # Make 2D convolution layer\n        # return layers.Conv2D(filters=out_planes, kernel_size=3, strides=stride, use_bias=False,\n        #                     kernel_initializer=kaiming_normal, name=name)(x)\n        return layers.Conv2D(filters=out_planes, kernel_size=3, strides=stride, use_bias=False,\n                             kernel_initializer=kaiming_normal, kernel_regularizer=keras.regularizers.L2(l2),\n                             name=name)(x)\n\n    def basic_block(x, planes, stride=1, downsample=None, name=None):\n        identity = x\n\n        out = conv3x3(x, planes, stride=stride, name=f'{name}.conv1')\n        out = layers.BatchNormalization(momentum=0.9, epsilon=1e-5, name=f'{name}.bn1')(out)\n        out = layers.ReLU(name=f'{name}.relu1')(out)\n\n        out = conv3x3(out, planes, name=f'{name}.conv2')\n        out = layers.BatchNormalization(momentum=0.9, epsilon=1e-5, name=f'{name}.bn2')(out)\n\n        if downsample is not None:\n            # Create an identical layer for each layer in the downsample\n            for layer in downsample:\n                identity = layer(identity)\n\n        # Performs element-wise addition of multiple inputs. It is used to combine or merge the outputs of two or more layers by adding them together\n        out = layers.Add(name=f'{name}.add')([identity, out])    \n        out = layers.ReLU(name=f'{name}.relu2')(out)\n\n        return out\n\n    def make_layer(x, planes, blocks, stride=1, name=None):\n        downsample = None\n\n        # inplanes refer to the number of channels in filters\n        inplanes = x.shape[3]\n\n        # Check whether we are downsampling our data (i.e. not going through every elememt)\n        # This happens under two circumstances:\n        # 1. when stride != 1\n        # 2. when the layer we want to make has no. of channels less than the no. of channels input has\n        if stride != 1 or inplanes != planes:\n            # downsample consists of a Conv2D layer and a BatchNormalization layer\n            downsample = [\n                layers.Conv2D(filters=planes, kernel_size=1, strides=stride, use_bias=False, kernel_initializer=kaiming_normal,\n                              kernel_regularizer=keras.regularizers.L2(l2),\n                              name=f'{name}.0.downsample.0'),\n#                    layers.Conv2D(filters=planes, kernel_size=1, strides=stride, use_bias=False, kernel_initializer=kaiming_normal,\n#                                  name=f'{name}.0.downsample.0'),\n                layers.BatchNormalization(momentum=0.9, epsilon=1e-5, name=f'{name}.0.downsample.1'),\n            ]\n\n        # If no downsample, downsample = None\n\n        x = basic_block(x, planes, stride, downsample, name=f'{name}.0')\n        for i in range(1, blocks):\n            x = basic_block(x, planes, name=f'{name}.{i}')\n\n        return x\n\n    def resnet(x, blocks_per_layer, num_classes=1000):\n\n        # ---------------------------------\n        # Initial entry block\n        x = layers.ZeroPadding2D(padding=3, name='conv1_pad')(x)\n        x = layers.Conv2D(filters=64, kernel_size=7, strides=2, use_bias=False,\n                          kernel_initializer=kaiming_normal,\n                          kernel_regularizer=keras.regularizers.L2(l2),\n                          name='conv1')(x)\n        # x = layers.Conv2D(filters=64, kernel_size=7, strides=2, use_bias=False,\n        #                  kernel_initializer=kaiming_normal,\n        #                  name='conv1')(x)\n        x = layers.BatchNormalization(momentum=0.9, epsilon=1e-5, name='bn1')(x)\n        x = layers.ReLU(name='relu1')(x)\n        x = layers.ZeroPadding2D(padding=1, name='maxpool_pad')(x)\n        x = layers.MaxPool2D(pool_size=3, strides=2, name='maxpool')(x)\n\n        # Additional - Addition of a dropout layer\n        if pooling_dropout:\n            x = layers.Dropout(rate = 0.2)(x)   \n        # ---------------------------------\n\n        # ---------------------------------\n        # This block of code creates the ResNet blocks\n        # In ResNet-18, only have 2 layers per block\n        x = make_layer(x, 64, blocks_per_layer[0], name='layer1')\n        x = make_layer(x, 128, blocks_per_layer[1], stride=2, name='layer2')\n        x = make_layer(x, 256, blocks_per_layer[2], stride=2, name='layer3')\n        x = make_layer(x, 512, blocks_per_layer[3], stride=2, name='layer4')\n        # ---------------------------------\n\n        # ---------------------------------\n        x = layers.GlobalAveragePooling2D(name='avgpool')(x)\n\n        # Addition of a dropout layer\n        if pooling_dropout:\n            x = layers.Dropout(rate = 0.2)(x)    \n\n        initializer = keras.initializers.RandomUniform(-1.0 / math.sqrt(512), 1.0 / math.sqrt(512))\n        x = layers.Dense(units=num_classes, kernel_initializer=initializer, bias_initializer=initializer, name='fc')(x)\n\n        # Additional - Addition of a dropout layer\n        if fc_dropout:\n            x = layers.Dropout(rate = 0.5)(x)  \n\n        # Softmax output layer\n        x = layers.Dense(units=num_classes, activation='softmax')(x)  \n        # ---------------------------------\n\n        return x\n\n    def resnet18(x, **kwargs):\n        return resnet(x, [2, 2, 2, 2], **kwargs)\n\n    def resnet34(x, **kwargs):\n        return resnet(x, [3, 4, 6, 3], **kwargs)\n\n    # Create model\n    model_inputs = keras.Input(shape = (193, 16, 1))\n    model_outputs = resnet18(model_inputs, num_classes = 3876)\n    resnet18_model = keras.Model(model_inputs, model_outputs)\n\n    # Compile model - Classification\n    # default learning rate is 1e-3 = 0.001\n    optimizer = tf.keras.optimizers.Adam(learning_rate = lr)\n\n    resnet18_model.compile(optimizer=optimizer,\n          loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n          metrics=['accuracy'])\n\n    return resnet18_model\n'''","metadata":{"execution":{"iopub.status.busy":"2023-06-13T06:54:41.635423Z","iopub.execute_input":"2023-06-13T06:54:41.635790Z","iopub.status.idle":"2023-06-13T06:54:41.665141Z","shell.execute_reply.started":"2023-06-13T06:54:41.635761Z","shell.execute_reply":"2023-06-13T06:54:41.664024Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Must change back to working directory for below code to run\n'''\nprint(os.getcwd())\nos.chdir('../../working')\nprint(os.getcwd())\n'''","metadata":{"execution":{"iopub.status.busy":"2023-06-13T06:40:51.759078Z","iopub.execute_input":"2023-06-13T06:40:51.759445Z","iopub.status.idle":"2023-06-13T06:40:51.765977Z","shell.execute_reply.started":"2023-06-13T06:40:51.759414Z","shell.execute_reply":"2023-06-13T06:40:51.764793Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Method 1\n'''\ntuner = kt.RandomSearch(\n    HyperModel(),\n    objective = kt.Objective(\"val_accuracy\", \"max\"),\n    max_trials = 30,\n    max_retries_per_trial = 3,\n    max_consecutive_failed_trials = 5\n)\n'''","metadata":{"execution":{"iopub.status.busy":"2023-06-13T13:02:06.063622Z","iopub.status.idle":"2023-06-13T13:02:06.063974Z","shell.execute_reply.started":"2023-06-13T13:02:06.063805Z","shell.execute_reply":"2023-06-13T13:02:06.063821Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Method 2\n'''\ntuner = kt.RandomSearch(build_model,\n                       objective = 'val_loss',\n                       max_trials = 30)\n'''","metadata":{"execution":{"iopub.status.busy":"2023-06-13T06:54:51.045033Z","iopub.execute_input":"2023-06-13T06:54:51.045413Z","iopub.status.idle":"2023-06-13T06:54:51.545572Z","shell.execute_reply.started":"2023-06-13T06:54:51.045383Z","shell.execute_reply":"2023-06-13T06:54:51.544639Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# tuner.search_space_summary()","metadata":{"execution":{"iopub.status.busy":"2023-06-13T06:54:56.792463Z","iopub.execute_input":"2023-06-13T06:54:56.792835Z","iopub.status.idle":"2023-06-13T06:54:56.799974Z","shell.execute_reply.started":"2023-06-13T06:54:56.792805Z","shell.execute_reply":"2023-06-13T06:54:56.798996Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\nstop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n\ntuner.search(X_train, y_train,\n             epochs = 100,\n             validation_data = (X_test, y_test),\n             callbacks = [stop_early]\n            )\n'''","metadata":{"execution":{"iopub.status.busy":"2023-06-13T06:55:16.447343Z","iopub.execute_input":"2023-06-13T06:55:16.447893Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\ntuner.results_summary()\n'''","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}